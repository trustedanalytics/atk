#!/usr/bin/env python2.7
## Copyright (c) 2015 Intel Corporation 
##
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.


"""
Requirements:
    - Cloudera's Python cm-api http://cloudera.github.io/cm_api/
    - Working Cloudera manager with at least a single cluster
    - Intel Analytics installation including the cluster-config RPM (which
        abstracts the Cloudera cm-api library to a higher level)
    - Sudo access (This script must be run as root or with root-level privileges)

This script queries Cloudera manager to get the host names of the machines running the following roles.
 -ZOOKEEPER server(the zookeeper role is actually called 'server')
 -HDFS name node

It also updates the spark-env.sh config in Cloudera manager with a necessary export of SPARK_CLASSPATH
needed for graph processing. The spark service config is re-deployed and the service is restarted.
If the Intel Analytics class path is already present no updates are done,
the config is not deployed and the spark service is not restarted.

CAUTION:
    You can run this script many times to pull the latest configuration details from Cloudera Manager but care should 
    be taken when configuring the database. If any existing database configurations exist
    it will be overwritten without warning. If you do change database configurations the REST server will loose
    all knowledge of any frames, graphs and other processed data that might have been created.

Command Line Arguments
    Every command line argument is required.

--host the cloudera Manager host address. If this script is run on host managed by Cloudera Manager we will try to get
    the host name from the Cloudera Manager API

--port the Cloudera Manager port. The port used to access the Cloudera Manager UI. (normally 7180 but no default is provided)

--username The Cloudera Manager user name. The user name for logging into Cloudera Manager

--password The Cloudera Manager password.  The user name for logging into Cloudera Manager

--cluster The Cloudera cluster we will create the config for. If Cloudera Manager manages more than one cluster
    we need to know what cluster we will be updating and pulling our config for. Can give the display name of the
    cluster

--db_only Configure only the database (yes/no).  If there is no existing cdh.conf file and db_only=yes, no cdh.conf file will be created.
    If there is an existing cdh.conf file and db_only=yes, the command-line parameters for connecting to Cloudera Manager will be used
    in preference to the contents of cdh.conf.  If the parameters given disagree with the contents of cdh.conf, the cdh.conf file will
    NOT be updated.  Any parameter for this option other than "yes" (case-insensitive) is treated as "no".

--db_host the hostname of your postgres database.

--db_port the port number for your postgres installation.

--db_database the postgres database name.

--db_username the database user name.

--db_password the database password.

--path  Directory to save new configuration files and read old ones (if present).
        Defaults to working directory

--log  Log level [INFO|DEBUG|WARNING|FATAL|ERROR]

"""

from __future__ import print_function
import os
import os.path
import sys
import argparse
import cluster_config as cc
from pyhocon import ConfigFactory
from cluster_config.const import Const
from cluster_config.cdh.cluster import Cluster, save_config, log
import cluster_config.cdh as cdh
import time, datetime
import hashlib, re, time, argparse, os, time, sys, getpass
import codecs
import subprocess
from pprint import pprint

IAUSER = "atkuser"

SPARK_USER = "spark"
#LIB_PATH = "/usr/lib/trustedanalytics/graphbuilder/lib/ispark-deps.jar"
IA_LOG_PATH = "/var/log/trustedanalytics/rest-server/output.log"
IA_START_WAIT_LOOPS = 30
IA_START_WAIT = 2
POSTGRES_WAIT = 3

CONF_DIR = "conf"
CLOUDERA_CONF_FILENAME      = 'cdh.conf'
DATABASE_CONF_FILENAME      = 'db.conf'
SPARK_CONF_FILENAME         = 'spark.conf'
USER_CUSTOM_CONF_FILENAME   = 'user.conf'

AUTOGENERATED_BOILERPLATE = """
##############################################################################
# WARNING! WARNING! WARNING! WARNING! WARNING! WARNING! WARNING! WARNING!    #
#                                                                            #
# This file is automatically generated.  Do not modify.  Any changes you     #
# make here WILL be overwritten during the process of configuring ATK.       #
#                                                                            #
# If you want to make any persistent customizations to ATK, create or edit   #
# a file called "user.conf" and add to it individual parameter assignments   #
# using the parameters provided in the file "application.conf.tpl".          #
#                                                                            #
# To avoid ambiguity each parameter in "user.conf" should be identified      #
# by its full "Java-config style" hierarchical name.  For example,           #
# "application.conf.tpl" contains the following section:                     #
#                                                                            #
#     trustedanalytics.atk {                                                 #
#       engine {                                                             #
#         auto-partitioner {                                                 #
#           # auto-partitioning spark based on the file size                 #
#           .                                                                #
#           .                                                                #
#           .                                                                #
#           # max-partitions is used if value is above the max upper-bound   #
#           max-partitions = 10000                                           #
#     ...                                                                    #
#                                                                            #
# If you want to set the parameter "max-partitions" in the "user.conf" file, #
# add a line to "user.conf" reading as follows:                              #
#                                                                            #
#     trustedanalytics.atk.engine.max-partitions = 15000                     #
#                                                                            #
# WARNING! WARNING! WARNING! WARNING! WARNING! WARNING! WARNING! WARNING!    #
##############################################################################
"""

class ConfigurationException(Exception):
    pass


def search_config(config_key, group_name, search_text):
    """
    centralize the config search since i was doing the same exact search on every config key lookup
    :param config_key: the config key from our application.conf
    :param group_name: The name of the regex group. makes it easy to find later
    :param search_text: the application.conf text to search in
    :return: the parameter as a string, or None if 'config_key' not found in 'search_text'
    """
    matches = re.search(r'' + config_key + ' = "(?P<' + group_name + '>.*)"', search_text)
    if matches:
        return matches.group(group_name)
    else:
        return None


def test_old_cdh_conf():
    """Check for an old cdh.conf file.  Unlike in the old
    (pre-OSS-release) version of the config script, there are no
    default values assumed for any of the configuration parameters.
    Whether there is an existing cdh.conf file or not, a new one will
    be written.  If new parameters are not passed in on the command
    line, we will generate warnings.
    """
    try:
        cdh_conf = codecs.open(CLOUDERA_CONF_FILENAME, encoding="utf-8", mode="r")
        cdh_conf_text = cdh_conf.read()
        cdh_conf.close()
    except:
        raise ConfigurationException("Missing or invalid " + CLOUDERA_CONF_FILENAME + " file")
    finally:
        return


def get_old_db_details():
    """Get the old database settings if we have any. Unlike in the old
    (pre-OSS-release) version of the config script, there are no
    default values assumed for any of the configuration parameters.
    If there is an existing db.conf file, parameters there are used,
    unless they are overridden on the command line.
    """
    db_conf_from_file = {}
    try:
        conf = ConfigFactory.parse_file('conf/db.conf')
        log.info("Using previous database configuration")
        db_conf_from_file = {
            'host':     conf.get("trustedanalytics.atk.metastore.connection-postgresql.host"),
            'port':     conf.get("trustedanalytics.atk.metastore.connection-postgresql.port"),
            'database': conf.get("trustedanalytics.atk.metastore.connection-postgresql.database"),
            'username': conf.get("trustedanalytics.atk.metastore.connection-postgresql.username"),
            'password': conf.get("trustedanalytics.atk.metastore.connection-postgresql.password") }
    except IOError:
        log.warning("no previous database configuration")

    return db_conf_from_file

def set_db_user_access(db_username):
    """
    set the postgres user access in pg_hba.conf file. We will only ever set localhost access. More open permissions
    will have to be updated by a system admin. The access ip rights gets appended to the top of the postgres conf
    file. repeated calls will keep appending to the same file.

    :param db_username: the database username
    """
    #update pg_hba conf file with user entry will only ever be for local host
    print('Configuring postgres access for user:' + db_username)
    try:
        pg_hba = codecs.open(r"/var/lib/pgsql/data/pg_hba.conf", encoding="utf-8", mode="r+")
    except IOError:
        try:
            os.system("service postgresql initdb")
            pg_hba = codecs.open(r"/var/lib/pgsql/data/pg_hba.conf", encoding="utf-8", mode="r+")
        except IOError:
            cc.log.fatal("Cannot initialize database or write to Postgres data file.\n"
                         "Check that you have superuser privileges.")

    pg_hba_text = pg_hba.read()
    pg_hba.seek(0)
    pg_hba.write("host    all         " + db_username + "      127.0.0.1/32            md5 #IATINSERT\n" + pg_hba_text)
    pg_hba.close()

    restart_db()


def create_db_user(db_username, db_password):
    """
    create the postgres user and set his password. Will do a OS system call to the postgres psql command to create the
    user.

    :param db_username: the  user name that will eventually own the database
    :param db_password: the password for the user
    """
    print(os.system("su -c \"echo \\\"create user " + db_username +
                 " with createdb encrypted password '" + db_password + "';\\\" | psql \"  postgres"))


def create_db(db, db_username):
    """
    Create the database and make db_username the owner. Does a system call to the postgres psql command to create the
    database

    :param db: the name of the database
    :param db_username: the postgres user that will own the database

    """
    print(os.system("su -c \"echo \\\"create database " + db + " with owner " + db_username + ";\\\" | psql \"  postgres"))


def create_IA_metauser(db):
    """
    Once postgres is configured and the IA server has been restarted we need to add the test user to so authentication
    will work in IA. Does a psql to set the record

    :param db: the database we will be inserting the record into

    """
    print(os.system("su -c \" echo \\\" \c " + db +
                 "; \\\\\\\\\  insert into users (username, api_key, created_on, modified_on) "
                 "values( 'metastore', 'test_api_key_1', now(), now() );\\\" | psql \" postgres "))


def restart_db():
    """
    We need to restart the postgres server for the access updates to pg_hba.conf take affect. I sleep right after to
    give the service some time to come up

    :return:
    """
    print(os.system("service postgresql  restart "))
    time.sleep(POSTGRES_WAIT)


def get_IA_log():
    """
    Open the output.log and save the contents to memory. Will be used monitor the IA server restart status.
    :return:
    """
    try:
        output_log = codecs.open(IA_LOG_PATH, encoding="utf-8", mode="r")
        output_log_text = output_log.read()
        output_log.close()
        return output_log_text
    except IOError:
        return ""


def restart_IA():
    """
    Send the linux service command to restart trustedanalytics analytics server and read the output.log file to see when the server
    has been restarted.
    :return:
    """
    #Truncate the IA log so we can detect a new 'Bound to' message which would let us know the server is up
    try:
        output_log = codecs.open(IA_LOG_PATH, encoding="utf-8", mode="w+")
        output_log.write("")
        output_log.close()
    except IOError:
        cc.log.info("Starting ATK...")
        log.info("Starting ATK...")

    #restart IA
    print(os.system("service trustedanalytics restart "))
    cc.log.info("Waiting for server to restart")
    log.info("Waiting for server to restart", GOOD)

    output_log_text = get_IA_log()
    count = 0
    #When we get the Bound to message the server has finished restarting
    while re.search("Bound to.*:.*", output_log_text) is None:
        print(" . ",)
        sys.stdout.flush()
        time.sleep(IA_START_WAIT)

        output_log_text = get_IA_log()

        count += 1
        if count > IA_START_WAIT_LOOPS:
            log.fatal("Intel Analytics Rest server didn't restart!")

    print("\n")


def set_db_details(db, db_username, db_password):
    """
    Update the local host Postgres install. Create the user, database and set network access.
    :param db: database name
    :param db_username: db user name
    :param db_password: db password

    """
    set_db_user_access(db_username)
    create_db_user(db_username, db_password)
    create_db(db, db_username)
    restart_db()
    restart_IA()
    create_IA_metauser(db)
    log.info("Postgres is configured.", GOOD)


def run_command(command):
    """
    Run an external command that may produce multiple lines of output and
    return the lines as an iterator so that the caller can choose whether to
    wait for the output or allow the subprocess to execute concurrently and
    check the lines intermittently.
    :param command: Commandline to be run.  Array or string.  If a string, automatically split into an array on space.
    :return: An iterator which contains the standard output of the command
    """
    if isinstance(command, str):
        command = command.split()
    p = subprocess.Popen(command,
                         stdout=subprocess.PIPE,
                         stderr=subprocess.STDOUT)
    return iter(p.stdout.readline, b'')


def setup_yarn_cluster_mode(cluster):
    log.info("Configuring for Spark on YARN-cluster mode...")

    # Sanity check that the database is running.
    command = "service postgresql status"
    for line in run_command(command):
        if not (line.find("postmaster") >= 0 and line.find("is running") >= 10):
            raise ConfigurationException("Database not running, cannot setup YARN cluster!")

    # Update pg_hba.conf for all the nodes in the cluster.
    yarn_internal_IPs = []
    for host in cluster.yarn.nodemanager.hosts.hosts:
        yarn_internal_IPs.append(cluster.yarn.nodemanager.hosts.ipAddresses())

    try:
        pg_hba_conf = codecs.open("/var/lib/pgsql/data/pg_hba.conf", encoding="utf-8", mode="a")
        for ip in yarn_internal_IPs:
            pg_hba_conf.write("host    all    all    " + ip + "/32    trust\n")
        log.info("YARN cluster is configured.", GOOD)
    except:
        raise ConfigurationException("Could not open '/var/lib/pgsql/data/pg_hba.conf' and write YARN server information")
    finally:
        pg_hba_conf.close()

    restart_db()
    restart_IA()


def cli(parser=None):
    current_db_details = get_old_db_details()
    if parser is None:
        parser = argparse.ArgumentParser(description="Configure a new ATK install")
        parser.add_argument("--db-only", type=str, help="configure only the database yes/no", default="no",
                            choices=["yes", "no"])
        parser.add_argument("--db-host", type=str, help="Database host name, default {0}".
                            format(current_db_details["host"] if "host" in current_db_details else "127.0.0.1"),
                            default=current_db_details["host"] if "host" in current_db_details else "127.0.0.1")
        parser.add_argument("--db-port", type=str, help="Database port number, default {0}".
                            format(current_db_details["port"] if "port" in current_db_details else 5432),
                            default=current_db_details["port"] if "port" in current_db_details else 5432)
        parser.add_argument("--db-database", type=str, help="Database name, default {0}".
                            format(current_db_details["database"] if "database" in current_db_details else None),
                            default=current_db_details["database"] if "database" in current_db_details else None)
        parser.add_argument("--db-username", type=str, help="Database username, default {0}".
                            format(current_db_details["username"] if "username" in current_db_details else None),
                            default=current_db_details["username"] if "username" in current_db_details else None)
        parser.add_argument("--db-password", type=str, help="Database password, default {0}".
                            format(current_db_details["password"] if "password" in current_db_details else None),
                            default=current_db_details["password"] if "password" in current_db_details else None)
    return parser


def create_config(identifier, prefix, confs, filename):
    try:
        configfile = codecs.open(filename, encoding="utf-8", mode="w")
        print(AUTOGENERATED_BOILERPLATE, file=configfile)
        for key in confs:
            print("{0}.{1}={2}".format(prefix, key, confs[key]), file=configfile)
        cc.log.info("Configuration created for Intel Analytics: {0} at {1}".format(identifier, filename))
    except:
        cc.log.fatal("Error creating or writing configuration file!")
        log.fatal("Error creating or writing configuration file: {0} at {1}".format(identifier, filename))
    finally:
        configfile.close()

def create_db_conf(conf_file_directory, db_conf):
    # Write new db.conf regardless of the --db_only parameter.
    create_config(identifier='database',
                  prefix='trustedanalytics.atk.metastore.connection-postgresql',
                  confs=db_conf, filename=conf_file_directory+DATABASE_CONF_FILENAME)

def create_cdh_conf(conf_file_directory, cdh_host, cdh_port, cdh_username, zookeeper_hosts, zookeeper_port):
    # Write out CDH file
    cdh_conf_to_write = {
        'fs.root': '"hdfs://{0}:{1}/user/{2}"'.format(cdh_host,
                                                    cdh_port,
                                                    cdh_username),
        'titan.load.storage.hostname': '"{0}"'.format(','.join(zookeeper_hosts)),
        'titan.load.storage.port': '"{0}"'.format(zookeeper_port) }
    create_config(identifier='CDH', prefix='trustedanalytics.atk.engine',
                  confs=cdh_conf_to_write, filename=conf_file_directory+CLOUDERA_CONF_FILENAME)

def create_spark_conf(conf_file_directory):
    spark_conf = {
        'spark.master': '"{0}"'.format('yarn-cluster') }
    create_config(identifier='SPARK', prefix='trustedanalytics.atk.engine',
                  confs=spark_conf, filename=conf_file_directory+SPARK_CONF_FILENAME)

def create_user_conf(conf_file_directory):

    if os.path.isfile(conf_file_directory) is False:
        spark_conf = {
            'api.host': '"{0}"'.format('127.0.0.1'),
            'api.port': 9099}
        create_config(identifier='SPARK', prefix='trustedanalytics.atk',
                      confs=spark_conf, filename=conf_file_directory+USER_CUSTOM_CONF_FILENAME)


"""
def create__conf(conf_file_directory, cdh_host, cdh_port, cdh_username, zookeeper_hosts, zookeeper_port):
    # Write out CDH file
    cdh_conf_to_write = {
        'fs.root': '"hdfs://{0}:{1}/user/{2}"'.format(cdh_host,
                                                      cdh_port,
                                                      cdh_username),
        'titan.load.storage.hostname': '"{0}"'.format(','.join(zookeeper_hosts)),
        'titan.load.storage.port': '"{0}"'.format(zookeeper_port) }
    create_config(identifier='CDH', prefix='trustedanalytics.atk.engine',
                  confs=cdh_conf_to_write, filename=conf_file_directory+CLOUDERA_CONF_FILENAME)
"""

def run(args, cluster=None):
    conf_file_directory = "{0}/{1}/".format(os.getcwd(), CONF_DIR)
    pprint(args)
    # Take what is on the commandline for CDH initial parameters, if anything.
    cdh_conf_from_commandline = {
        'host':     args.db_host     if args.db_host     else None,
        'port':     args.db_port     if args.db_port     else None,
        'database': args.db_database if args.db_database else None,
        'username': args.db_username if args.db_username else None,
        'password': args.db_password if args.db_password else None,
        'url': '"jdbc:postgresql://"${trustedanalytics.atk.metastore.connection-postgresql.host}":"${trustedanalytics.atk.metastore.connection-postgresql.port}"/"${trustedanalytics.atk.metastore.connection-postgresql.database}' }

    # Query Cloudera for the parameters we can get from it using the
    # config_cluster API.  Login and password must be provided on the
    # commandline.
    if cluster is None:
        cluster = Cluster(args.host, args.port, args.username, args.password, args.cluster)
    cdh_conf = {}
    zookeeper_client_port = cluster.zookeeper.server.server_base.clientport.get()
    cdh_conf['port'] = cluster.hdfs.namenode.namenode_base.namenode_port.get()
    cdh_conf['host'] = cluster.hdfs.namenode.hosts.hostnames()[0]


    cdh_conf.update(cdh_conf_from_commandline)

    if args.db_only == 'yes':
        try:
            test_old_cdh_conf()
        except Exception as e:
            cc.log.warning(e)
            log.warning("{0} \nDid you mean to create a new CDH configuration file?".format(e))
        else:
            cc.log.warning("Existing {0} file being used. Did you mean to create a new one?".
                           format(CLOUDERA_CONF_FILENAME))

        create_db_conf(conf_file_directory, cdh_conf_from_commandline)

        set_db_details(cdh_conf_from_commandline['database'], cdh_conf_from_commandline['username'], cdh_conf_from_commandline['password'])

        setup_yarn_cluster_mode(cluster)


    else:
        log.info("New {0} file will be written.".format(CLOUDERA_CONF_FILENAME))
        for key in cdh_conf:
            if cdh_conf[key] is None:
                # Sanity check.  We should never get here.
                cc.log.fatal("Missing {0} parameter for main CDH config.\n Cannot write new file!".format(key))

        create_db_conf(conf_file_directory, cdh_conf_from_commandline)

        create_cdh_conf(conf_file_directory, cdh_conf['host'], cdh_conf['port'], cdh_conf['username'],
                        cluster.yarn.nodemanager.hosts.hostnames(),
                        cluster.zookeeper.server.server_base.clientport.get())

        create_spark_conf(conf_file_directory)

        set_db_details(cdh_conf_from_commandline['database'], cdh_conf_from_commandline['username'], cdh_conf_from_commandline['password'])

        setup_yarn_cluster_mode(cluster)

        create_user_conf(conf_file_directory)

    log.info("Intel Analytics is ready for use.")



    sys.exit(1)
    # Handle the db_only option.
    if args.db_only.lower().strip() == 'yes':
        # User wants to just write a new database config and leave CDH config unchanged.
        try:
            test_old_cdh_conf()
        except Exception as e:
            cc.log.warning(e)
            log.warning(e + "\nDid you mean to create a new CDH configuration file?", WARNING)
        else:
            cc.log.warning("Existing " + CLOUDERA_CONF_FILENAME + " file being used.")
    else:   # db_only == 'no'
        cc.log.warning("New " + CLOUDERA_CONF_FILENAME + " file will be written.")
        for key in cdh_conf:
            if cdh_conf[key] is None:
                # Sanity check.  We should never get here.
                cc.log.fatal("Missing '" + key + "' parameter for main CDH config.\n" +
                             "Cannot write new file!")
        # Write out CDH file
        cdh_conf_to_write = {
            'fs.root': "hdfs://{0}:{1}/user/{2}".format(cdh_conf['host'],
                                                        cdh_conf['port'],
                                                        cdh_conf['username']),
            'titan.load.storage.hostname': '"' + ','.join(cluster.yarn.nodemanager.hosts.hostnames()) + '"',
            'titan.load.storage.port': '"' + zookeeper_client_port + '"' }
        create_config(identifier='CDH', prefix='trustedanalytics.atk.engine',
                      confs=cdh_conf_to_write, filename=conf_file_directory+CLOUDERA_CONF_FILENAME)

    # Write new db.conf regardless of the --db_only parameter.
    create_config(identifier='database',
                  prefix='trustedanalytics.atk.metastore.connection-postgresql',
                  confs=db_conf, filename=conf_file_directory+DATABASE_CONF_FILENAME)

    set_db_details(db_conf['database'], db_conf['username'], db_conf['password'])

    setup_yarn_cluster_mode(cluster)

    os.system("touch " + conf_file_directory + USER_CUSTOM_CONF_FILENAME)




if __name__ == '__main__':
    run(cc.cli.parse(cli()))
