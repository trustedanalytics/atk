# This (application.conf.tpl) is a configuration template for the Trusted Analytics Toolkit.
# Copy this to application.conf and edit to suit your system.
# Comments begin with a '#' character.
# Default values are 'commented' out with //.
# To configure for your system, look for configuration entries below with the word
# REQUIRED in all capital letters - these
# MUST be configured for the system to work.
trustedanalytics.atk.engine.hadoop.kerberos {
          enabled = ${KERBEROS_ENABLED}
}

# BEGIN REQUIRED SETTINGS
trustedanalytics.atk {

  engine.hadoop.configuration.path="/etc/hadoop/conf:/home/vcap/app/conf"
  engine.hbase.configuration.path="/etc/hbase/conf:/home/vcap/app/conf"

  # dynamic libraries for Intel Data Analytics Acceleration Library (Intel DAAL)
  engine.spark.daal.dynamic-libraries=${DAAL_LIB_DIR}"/libAtkDaalJavaAPI.so,"${DAAL_LIB_DIR}"/libiomp5.so,"${DAAL_LIB_DIR}"/libJavaAPI.so,"${DAAL_LIB_DIR}"/"${DAAL_GCC_VERSION}"/libtbb.so.2"

  //engine.logging.raw = true
  //engine.logging.profile = true
  //api.logging.raw = true
  //api.logging.profile = true

  #bind address - change to 0.0.0.0 to listen on all interfaces
  api.host = "0.0.0.0"

  #bind port
  //api.port = 9099
  api.port = ${PORT}
  api.request-timeout = 60s

  metastore
    {
      # Postgresql
      connection-postgresql.host = ${PG_HOST}
      connection-postgresql.port = ${PG_PORT}
      connection-postgresql.database = ${PG_DB}
      connection-postgresql.username = ${PG_USER}
      connection-postgresql.password = ${PG_PASS}
      connection-postgresql.url = ${trustedanalytics.atk.jdbc.prefix}${trustedanalytics.atk.jdbc.url.splitter}${trustedanalytics.atk.postgres.prefix}${trustedanalytics.atk.jdbc.url.splitter}"//"${trustedanalytics.atk.metastore.connection-postgresql.host}${trustedanalytics.atk.jdbc.url.splitter}${trustedanalytics.atk.metastore.connection-postgresql.port}"/"${trustedanalytics.atk.metastore.connection-postgresql.database}"?user="${PG_USER}"&password="${PG_PASS}

      # This allows for the use of postgres for a metastore. Service restarts will not affect the data stored in postgres
      connection = ${trustedanalytics.atk.metastore.connection-postgresql}
    }

  datastore
    {
    # Postgresql
    connection-postgresql.host = ${POSTGRES_HOST}
    connection-postgresql.port = ${POSTGRES_PORT}
    connection-postgresql.database = ${POSTGRES_DB}
    connection-postgresql.username = ${POSTGRES_USER}
    connection-postgresql.password = ${POSTGRES_PASS}
    connection-postgresql.url = ${trustedanalytics.atk.jdbc.prefix}${trustedanalytics.atk.jdbc.url.splitter}${trustedanalytics.atk.postgres.prefix}${trustedanalytics.atk.jdbc.url.splitter}"//"${trustedanalytics.atk.datastore.connection-postgresql.host}${trustedanalytics.atk.jdbc.url.splitter}${trustedanalytics.atk.datastore.connection-postgresql.port}"/"${trustedanalytics.atk.datastore.connection-postgresql.database}"?user="${POSTGRES_USER}"&password="${POSTGRES_PASS}

    # MYSQL
    connection-mysql.host = ${MYSQL_HOST}
    connection-mysql.port = ${MYSQL_PORT}
    connection-mysql.database = ${MYSQL_DB}
    connection-mysql.username = ${MYSQL_USER}
    connection-mysql.password = ${MYSQL_PASS}
    connection-mysql.url = ${trustedanalytics.atk.jdbc.prefix}${trustedanalytics.atk.jdbc.url.splitter}${trustedanalytics.atk.mysql.prefix}${trustedanalytics.atk.jdbc.url.splitter}"//"${trustedanalytics.atk.datastore.connection-mysql.host}${trustedanalytics.atk.jdbc.url.splitter}${trustedanalytics.atk.datastore.connection-mysql.port}"/"${trustedanalytics.atk.datastore.connection-mysql.database}"?user="${MYSQL_USER}"&password="${MYSQL_PASS}

    # SQLSERVER
    connection-sqlserver.host = ""
    connection-sqlserver.port = ""
    connection-sqlserver.database = ""
    connection-sqlserver.username = ""
    connection-sqlserver.password = ""
    connection-sqlserver.url = ""

  }

    # This allows the use of an in memory data store. Restarting the rest server will create a fresh database and any
    # data in the h2 DB will be lost
    //metastore.connection = ${trustedanalytics.atk.metastore.connection-h2}

    engine {

      # The hdfs URL where the trustedanalytics folder will be created
      # and which will be used as the starting point for any relative URLs
      fs.root = ${FS_ROOT}"/user/atkuser/"${APP_NAME}
      //fs.root = ${FS_ROOT}
      //fs.root = "/home/vcap/app"

      # The URL for connecting to the Spark master server
      //spark.master = "spark://ip-10-10-9-164.us-west-2.compute.internal:7077"
      //spark.master = ${SPARK_MASTER}
      spark.master = "yarn-cluster"
      spark.home = "/opt/cloudera/parcels/CDH/lib/spark"

      #akka.remote.untrusted-mode = on
      #akka.remote.netty.tcp.hostname = ${SPARK_DRIVER_HOST}
      #akka.remote.netty.tcp.hostname = 0.0.0.0
      #akka.remote.netty.tcp.bind-hostname = 0.0.0.0
      spark.conf.properties {

        # Memory should be same or lower than what is listed as available in Cloudera Manager.
        # Values should generally be in gigabytes, e.g. "8g"
        spark.executor.memory = "2g"
        spark.eventLog.overwrite= true
        spark.eventLog.enabled = true
        spark.eventLog.dir = ${SPARK_EVENT_LOG_DIR}

        # Preferably spark.yarn.jar is installed in HDFS
        # In Cloudera Manager,
        #   1) Make sure the SPARK setting "spark_jar_hdfs_path" is set to this value
        #   2) Use "Actions" -> "Upload Spark Jar" to install jar in HDFS, if it is not already there
        spark.yarn.jar=${FS_ROOT}"/user/spark/share/lib/spark-assembly.jar"
        
        # Allow user defined functions (UDF's) to be overwritten
        spark.files.overwrite = true
        
        #uncomment the following config variables for a spark customized environment
        //spark.executor.uri = ${SPARK_DRIVER_HOST}
        //spark.driver.host = 0.0.0.0
        //spark.driver.port = ${SPARK_DRIVER_PORT}
        //spark.fileserver.port = ${SPARK_FILESERVER_PORT}
        //spark.broadcast.port = ${SPARK_BROADCAST_PORT}
        //spark.replClassServer.port = ${SPARK_REPL_CLASS_SERVER_PORT}
        //spark.blockManager.port = ${SPARK_BLOCK_MANAGER_PORT}
        //spark.executor.port = ${SPARK_EXECUTOR_PORT}
        //spark.ui.enabled = false
        #spark.ui.port = ${SPARK_UI_PORT}
        //spark.akka.timeout=60000
        //akka.remote.untrusted-mode = on
        #akka.remote.netty.tcp.hostname = ${SPARK_DRIVER_HOST}
        #akka.remote.netty.tcp.bind-hostname = 0.0.0.0
        #akka.remote.netty.tcp.hostname = 0.0.0.0
        //spark.storage.blockManagerHeartBeatMs=600000
        //spark.storage.blockManagerSlaveTimeoutMs=600000
        //spark.worker.timeout=60000

      }
    }


  }

