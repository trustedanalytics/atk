
# The configuration parameters are documented at https://github.com/thinkaurelius/titan/wiki/Using-HBase

#storage.hostname=NEEDS TO BE CONFIGURED
#storage.hbase.table=NEEDS TO BE CONFIGURED
#storage.cassandra.keyspace=NEEDS TO BE CONFIGURED

# Size of the id block to be acquired for Titan. Larger block sizes require fewer block applications but also
# leave a larger fraction of the id pool occupied and potentially lost. For write heavy applications, larger
# block sizes should be chosen.
#
# Rule of thumb: Set ids.block-size to the number of vertices you expect to add per Titan instance per hour.
# https://github.com/thinkaurelius/titan/wiki/Bulk-Loading
#
# 10,000 is Titan default.
# 600,000 is better for very large graphs.
#
ids.block-size=100000

# Number of partitions for Titan ID allocation pool
ids.num-partitions=10


# Number of milliseconds Titanâ€™s id pool manager will wait while attempting to acquire a new id block before
# failing.
#
# Rule of thumb: Set this value to be larger than 2 x (storage.idauthority-retries) x
# (storage.idauthority-wait-time). The only downside of increasing it is that Titan will try for a long time
# on an unavailable storage backend cluster.
#
# 60,000 is Titan default.
# We've used numbers as large as 2,400,000 for very large graphs.
#
ids.renew-timeout=150000


# Number of milliseconds that Titan will wait before re-attempting a failed backend operation. A higher value
# can ensure that operation re-tries do not further increase the load on the backend.
#
# 250 milliseconds is Titan default.
# 1000 would be a high value.
#
# Larger numbers slow down the whole process but this can be good because it gives HBase a chance to keep up.
#
# Large graphs need a higher value.
#
storage.attempt-wait=250

# storage.backend parameter for Titan. Currently GB 2.0 (alpha) supports only HBase as the storage backend.
storage.backend=hbase

# Enables batch loading which improves write performance but assumes that only one thread is interacting with
# the graph and that vertices retrieved by id exist. Under these assumptions locking and some read operations
# can be avoided. Furthermore, the configured storage backend will make backend specific configurations that
# facilitate loading performance
#
# Needs to be true for large graphs.
#
storage.batch-loading=true

storage.connection.timeout=100000

# Number of times the system attempts to acquire a unique id block before giving up and throwing an exception.
#
# Rule of thumb: Increase this value. The only downside of increasing it is that Titan will try for a long
# time on an unavailable storage backend cluster.
#
# 20 is Titan default.
#
storage.idauthority-retries=25

# The number of milliseconds the system waits for an id block application to be acknowledged by the storage
# backend in Titan.  The shorter this time, the more likely it is that an application will fail on a congested
# storage cluster.
#
# Rule of thumb: Set this to the sum of the 95th percentile read and write times measured on the storage
# backend cluster under load.
#
# Important: This value should be the same across all Titan instances.
#
# 300 is Titan default.
#
storage.idauthority-wait-time=300

# 2181 is Titan default.
storage.port=2181

# How many times Titan will attempt to execute a write operation against the storage backend before giving up.
# If it is expected that there is a high load on the backend during bulk loading, it is generally advisable to
# increase these configuration options.
#
# 5 is Titan default.
#
storage.write-attempts=10
