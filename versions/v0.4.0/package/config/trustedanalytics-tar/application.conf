# This (application.conf.tpl) is a configuration template for the Trusted Analytics Toolkit.
# Copy this to application.conf and edit to suit your system.
# Comments begin with a '#' character.
# Default values are 'commented' out with //.
# To configure for your system, look for configuration entries below with the word
# REQUIRED in all capital letters - these
# MUST be configured for the system to work.
//trustedanalytics.atk.engine.hadoop.kerberos {
//          enabled =false 
//          principal-name =${PRINCIPAL} 
//          keytab-file = ${KEYTAB}
//        }

# BEGIN REQUIRED SETTINGS

trustedanalytics.atk {

 engine.hadoop.configuration.path="/etc/hadoop/conf:/home/vcap/app/conf"
 engine.hbase.configuration.path="/etc/hbase/conf:/home/vcap/app/conf"
  
  //  engine.logging.raw = true
  // engine.logging.profile = true
  //api.logging.raw = true
  //api.logging.profile = true

  #bind address - change to 0.0.0.0 to listen on all interfaces
  api.host = "0.0.0.0"

  #bind port
  //api.port = 9099
  api.port = ${PORT}
  api.request-timeout = 60s

  #jdbc connector section
  jdbc.prefix = "jdbc"
  jdbc.url.splitter = ":"

  postgres.prefix = "postgresql"
  mysql.prefix = "mysql"
  sqlserver.prefix = "sqlserver"

  # The host name for the Postgresql database in which the metadata will be stored
  metastore.connection-postgresql.host = ${PG_HOST}
  metastore.connection-postgresql.port = ${PG_PORT}
  metastore.connection-postgresql.database = ${PG_DB}
  metastore.connection-postgresql.username = ${PG_USER}
  metastore.connection-postgresql.password = ${PG_PASS}
  metastore.connection-postgresql.url = ${jdbc.prefix}${jdbc.url.splitter}${postgres.prefix}${jdbc.url.splitter}"//"${trustedanalytics.atk.metastore.connection-postgresql.host}${jdbc.url.splitter}${trustedanalytics.atk.metastore.connection-postgresql.port}"/"${trustedanalytics.atk.metastore.connection-postgresql.database}"?user="${PG_USER}"&password="${PG_PASS}

  # This allows for the use of postgres for a metastore. Service restarts will not affect the data stored in postgres
  metastore.connection = ${trustedanalytics.atk.metastore.connection-postgresql}

  #jdbc urls
  postgres.url = ${metastore.connection-postgresql.url}

  # This allows the use of an in memory data store. Restarting the rest server will create a fresh database and any
  # data in the h2 DB will be lost
  //metastore.connection = ${trustedanalytics.atk.metastore.connection-h2}

  engine {

    # The hdfs URL where the trustedanalytics folder will be created
    # and which will be used as the starting point for any relative URLs
    //fs.root = ${FS_ROOT}
    fs.root = ${FS_ROOT}"/"${APP_NAME}
    //fs.root = "/home/vcap/app"

    # The (comma separated, no spaces) Zookeeper hosts that
    # Comma separated list of host names with zookeeper role assigned
    titan.load.storage.hostname = ${ZOOKEEPER_HOST}
    # Zookeeper client port, defaults to 2181
    titan.load.storage.port = ${ZOOKEEPER_PORT}

    titan.query.storage.hostname = ${ZOOKEEPER_HOST}

    # The URL for connecting to the Spark master server
    //spark.master = "spark://ip-10-10-9-164.us-west-2.compute.internal:7077"
    //spark.master = ${SPARK_MASTER}
    spark.master = "yarn-cluster"
    spark.home = "/opt/cloudera/parcels/CDH/lib/spark"

    #akka.remote.untrusted-mode = on
    #akka.remote.netty.tcp.hostname = ${SPARK_DRIVER_HOST}
    #akka.remote.netty.tcp.hostname = 0.0.0.0
    #akka.remote.netty.tcp.bind-hostname = 0.0.0.0
    spark.conf.properties {
      # Memory should be same or lower than what is listed as available in Cloudera Manager.
      # Values should generally be in gigabytes, e.g. "8g"
      spark.executor.memory = "2g"
      spark.eventLog.overwrite= true
      spark.eventLog.enabled = true
      spark.eventLog.dir = ${SPARK_EVENT_LOG_DIR}

      #uncomment the following config variables for a spark customized environment
      //spark.executor.uri = ${SPARK_DRIVER_HOST}
      //spark.driver.host = 0.0.0.0
      //spark.driver.port = ${SPARK_DRIVER_PORT}
      //spark.fileserver.port = ${SPARK_FILESERVER_PORT}
      //spark.broadcast.port = ${SPARK_BROADCAST_PORT}
      //spark.replClassServer.port = ${SPARK_REPL_CLASS_SERVER_PORT}
      //spark.blockManager.port = ${SPARK_BLOCK_MANAGER_PORT}
      //spark.executor.port = ${SPARK_EXECUTOR_PORT}
      //spark.ui.enabled = false
      #spark.ui.port = ${SPARK_UI_PORT}
      //spark.akka.timeout=60000
      //akka.remote.untrusted-mode = on
      #akka.remote.netty.tcp.hostname = ${SPARK_DRIVER_HOST}
      #akka.remote.netty.tcp.bind-hostname = 0.0.0.0
      #akka.remote.netty.tcp.hostname = 0.0.0.0
      //spark.storage.blockManagerHeartBeatMs=600000
      //spark.storage.blockManagerSlaveTimeoutMs=600000
      //spark.worker.timeout=60000

    }
  }

}

