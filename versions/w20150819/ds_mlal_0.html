<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8">
    
    <title>Machine Learning Algorithms &mdash; Trusted Analytics Package 1.1.0 documentation</title>
    
    <link rel="stylesheet" type="text/css" href="f_static/css/spc-bootstrap.css">
    <link rel="stylesheet" type="text/css" href="f_static/css/spc-extend.css">
    <link rel="stylesheet" href="f_static/scipy.css" type="text/css" >
    <link rel="stylesheet" href="f_static/pygments.css" type="text/css" >
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="f_static/jquery.js"></script>
    <script type="text/javascript" src="f_static/underscore.js"></script>
    <script type="text/javascript" src="f_static/doctools.js"></script>
    <script type="text/javascript" src="f_static/js/copybutton.js"></script>
    <link rel="index" title="Index" href="genindex.html" >
    <link rel="top" title="Trusted Analytics Package 1.1.0 documentation" href="index.html" >
    <link rel="up" title="Machine Learning" href="ds_ml.html" >
    <link rel="next" title="Best Known Methods (User)" href="ds_bkm.html" >
    <link rel="prev" title="Machine Learning" href="ds_ml.html" > 
  </head>
  <body>

  <div class="container">
    <div class="header">
    </div>
  </div>


    <div class="container">
      <div class="main">
        
	<div class="row-fluid">
	  <div class="span12">
	    <div class="spc-navbar">
              
    <ul class="nav nav-pills pull-left">
	
        <li class="active"><a href="index.html">Trusted Analytics</a></li>
	
          <li class="active"><a href="ds_over.html" >User Manual</a></li>
          <li class="active"><a href="ds_ml.html" accesskey="U">Machine Learning</a></li> 
    </ul>
              
              
    <ul class="nav nav-pills pull-right">
      <li class="active">
        <a href="ds_ml.html" title="Machine Learning"
           accesskey="P">previous</a>
      </li>
      <li class="active">
        <a href="ds_bkm.html" title="Best Known Methods (User)"
           accesskey="N">next</a>
      </li>
      <li class="active">
        <a href="genindex.html" title="General Index"
           accesskey="I">index</a>
      </li>
    </ul>
              
	    </div>
	  </div>
	</div>
        

	<div class="row-fluid">
      <div class="spc-rightsidebar span3">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
<h3><a href="index.html">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Technical Summary</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="ds_over.html">User Manual</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ds_strt.html">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="ds_dflw.html">Process Flow Examples</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="ds_ml.html">Machine Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="ds_bkm.html">Best Known Methods (User)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="dev_over.html">Extending Trusted Analytics</a></li>
<li class="toctree-l1"><a class="reference internal" href="ad_over.html">Administration</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="rest_api/v1/index.html">REST API</a></li>
</ul>
<ul class="simple">
</ul>

        </div>
      </div>
          <div class="span9">
            
        <div class="bodywrapper">
          <div class="body" id="spc-section-body">
            
  <div class="section" id="machine-learning-algorithms">
<span id="index-0"></span><h1>Machine Learning Algorithms<a class="headerlink" href="#machine-learning-algorithms" title="Permalink to this headline">¶</a></h1>
<div class="contents local topic" id="table-of-contents">
<p class="topic-title first">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#collaborative-filtering" id="id11">Collaborative Filtering</a><ul>
<li><a class="reference internal" href="#using-als-optimization-to-solve-the-collaborative-filtering-problem" id="id12">Using ALS Optimization to Solve the Collaborative Filtering Problem</a></li>
<li><a class="reference internal" href="#matrix-factorization-based-on-conjugate-gradient-descent-cgd" id="id13">Matrix Factorization based on Conjugate Gradient Descent (CGD)</a><ul>
<li><a class="reference internal" href="#the-mathematics-of-matrix-factorization-via-cgd" id="id14">The Mathematics of Matrix Factorization via CGD</a></li>
<li><a class="reference internal" href="#comparison-between-cgd-and-als" id="id15">Comparison between CGD and ALS</a><ul>
<li><a class="reference internal" href="#usage" id="id16">Usage</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#graphical-models" id="id17">Graphical Models</a></li>
<li><a class="reference internal" href="#topic-modeling" id="id18">Topic Modeling</a><ul>
<li><a class="reference internal" href="#topic-modeling-with-latent-dirichlet-allocation" id="id19">Topic Modeling with Latent Dirichlet Allocation</a><ul>
<li><a class="reference internal" href="#the-typical-lda-workflow" id="id20">The Typical <abbr title="Latent Dirichlet Allocation">LDA</abbr> Workflow</a></li>
<li><a class="reference internal" href="#data-preparation-and-ingest" id="id21">Data preparation and ingest</a></li>
<li><a class="reference internal" href="#assignment-to-training-or-testing-partition" id="id22">Assignment to training or testing partition</a></li>
<li><a class="reference internal" href="#graph-construction" id="id23">Graph construction</a></li>
<li><a class="reference internal" href="#training-lda" id="id24">Training <abbr title="Latent Dirichlet Allocation">LDA</abbr></a></li>
<li><a class="reference internal" href="#evaluation" id="id25">Evaluation</a></li>
<li><a class="reference internal" href="#interpretation-of-results" id="id26">Interpretation of results</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<p>The graph machine learning algorithms currently supported fall into these
categories:
<em>Collaborative Filtering</em>, <em>Graphical Models</em>, and <em>Topic Modeling</em>.</p>
<div class="section" id="collaborative-filtering">
<h2>Collaborative Filtering<a class="headerlink" href="#collaborative-filtering" title="Permalink to this headline">¶</a></h2>
<p>See the <a class="reference external" href="python_api/models/model-collaborative_filtering/index.html">models section of the API</a> for details.</p>
<ul>
<li><dl class="first docutils">
<dt><a class="reference internal" href="#graphical-models">Graphical_Models</a></dt>
<dd><ul class="first last simple">
<li><a href="#id27"><span class="problematic" id="id28">`LP`_</span></a></li>
<li><a href="#id29"><span class="problematic" id="id30">`LBP`_</span></a></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a class="reference internal" href="#topic-modeling">Topic_Modeling</a></dt>
<dd><ul class="first last simple">
<li><a class="reference internal" href="#lda">LDA</a></li>
</ul>
</dd>
</dl>
</li>
</ul>
<span class="target" id="als"></span><div class="section" id="using-als-optimization-to-solve-the-collaborative-filtering-problem">
<span id="index-1"></span><h3>Using ALS Optimization to Solve the Collaborative Filtering Problem<a class="headerlink" href="#using-als-optimization-to-solve-the-collaborative-filtering-problem" title="Permalink to this headline">¶</a></h3>
<p>ALS optimizes the vector <img class="math" src="f_images/math/7c1fb8ead0f64fa9efd46cb5083349f35f6a117c.png" alt="\overrightarrow f_{*}"/> and the bias
<img class="math" src="f_images/math/c15d5cd6fbe29422a247d0885e3703344ceeeb7f.png" alt="b_{*}"/> alternatively between user profiles using least squares on users
and items.
On the first iteration, the first feature of each item is set to its average
rating, while the others are set to small random numbers.
The algorithm then treats the <img class="math" src="f_images/math/c4bb40dd65eae6c11b325989b14e0b8d35e4e3ef.png" alt="m"/> &#8216;s as constant and optimizes
<img class="math" src="f_images/math/7c6e313e6c7ed81119cf745cc9664f51fb405138.png" alt="u_{i}^{1},…,u_{i}^{k}"/> for each user, <img class="math" src="f_images/math/a581f053bbfa5115f42c13094857cdd12a37ec49.png" alt="i"/>.
For an individual user, this is a simple ordinary least squares optimization
over the items that user has ranked.
Next, the algorithm takes the <img class="math" src="f_images/math/e5fc41b391867da81606413e3389c7efc73abaf0.png" alt="u"/> ’s as constant and optimizes the
<img class="math" src="f_images/math/df979c850990b5618a442b268beb6dff1308fe87.png" alt="m_{j}^{1},…,m_{j}^{k}"/> for each item, <img class="math" src="f_images/math/d32c78b759903e3f4bd4fd2ce0b86358f7500c5d.png" alt="j"/>.
This is again an ordinary least squares optimization predicting the user
rating of person that has ranked item <img class="math" src="f_images/math/d32c78b759903e3f4bd4fd2ce0b86358f7500c5d.png" alt="j"/>.</p>
<p>At each step, the bias is computed for either items of users and the objective
function, shown below, is evaluated.
The bias term for an item or user, computed for use in the next iteration is
given by:</p>
<div class="math">
<p><img src="f_images/math/9b60dd2a4e416becc36ef9065cdade434c4d3dca.png" alt="b = \frac{\sum error}{(1+\lambda)*n}"/></p>
</div><p>The optimization is said to converge if the change in the objective function
is less than the convergence_threshold parameter or the algorithm hits the
maximum number of <a class="reference internal" href="glossary.html#term-supersteps"><span class="xref std std-term">supersteps</span></a>.</p>
<div class="math">
<p><img src="f_images/math/e400a815345484e467f59bc6b8621d7a0075aca6.png" alt="cost = \frac {\sum error^{2}}{n}+\lambda*\left(bias^{2}+\sum f_{k}^{2} \
\right)"/></p>
</div><p>Note that the equations above omit user and item subscripts for generality.
The <img class="math" src="f_images/math/74ba80b12448019d7d7e404258d2dc8d0d218e3e.png" alt="l_{2}"/> regularization term, lambda, tries to avoid overfitting by
penalizing the magnitudes of the parameters, and <img class="math" src="f_images/math/1ab0134b6e0837594649c75a2ed83cfd85a2d03d.png" alt="\lambda"/> is a tradeoff
parameter that balances the two terms and is usually determined by cross
validation (CV).</p>
<p>After the parameters <img class="math" src="f_images/math/7c1fb8ead0f64fa9efd46cb5083349f35f6a117c.png" alt="\overrightarrow f_{*}"/> and <img class="math" src="f_images/math/c15d5cd6fbe29422a247d0885e3703344ceeeb7f.png" alt="b_{*}"/> are
determined, given an item <img class="math" src="f_images/math/52824eea6fecfe5c51900c2cccf0fdfa617dd5dc.png" alt="m_{j}"/> the rating from user <img class="math" src="f_images/math/6998bb57f6af36a0ac188d683da16738d6c24131.png" alt="u_{i}"/> can
be predicted by the simple linear model:</p>
<div class="math">
<p><img src="f_images/math/5b347b3f38e831ff7d91429a54decf0351986662.png" alt="r_{ij} = \overrightarrow {f_{ui}} \cdot \overrightarrow {f_{mj}} + b_{ui} \
+ b_{mj}"/></p>
</div><span class="target" id="cgd"></span></div>
<div class="section" id="matrix-factorization-based-on-conjugate-gradient-descent-cgd">
<span id="index-2"></span><h3>Matrix Factorization based on Conjugate Gradient Descent (CGD)<a class="headerlink" href="#matrix-factorization-based-on-conjugate-gradient-descent-cgd" title="Permalink to this headline">¶</a></h3>
<p>This is the Conjugate Gradient Descent (CGD) with Bias for collaborative
filtering algorithm.
Our implementation is based on the paper:</p>
<p>Y. Koren. Factorization Meets the Neighborhood: a Multifaceted Collaborative
Filtering Model.
In ACM KDD 2008. (Equation 5)
<a class="reference external" href="http://public.research.att.com/~volinsky/netflix/kdd08koren.pdf">http://public.research.att.com/~volinsky/netflix/kdd08koren.pdf</a></p>
<p>This algorithm for collaborative filtering is used in <a class="reference internal" href="glossary.html#term-recommendation-systems"><span class="xref std std-term">recommendation
systems</span></a> to suggest items (products, movies, articles, and so on) to potential
users based on historical records of items that all users have purchased,
rated, or viewed.
The records are usually organized as a preference matrix P, which is a sparse
matrix holding the preferences (such as, ratings) given by users to items.
Similar to ALS, CGD falls in the category of matrix factorization/latent factor
model that infers user profiles and item profiles in low-dimension space, such
that the original matrix P can be approximated by a linear model.</p>
<p>This factorization method uses the conjugate gradient method for its
optimization subroutine.
For more on conjugate gradient descent in general, see:
<a class="reference external" href="http://en.wikipedia.org/wiki/Conjugate_gradient_method">http://en.wikipedia.org/wiki/Conjugate_gradient_method</a>.</p>
<div class="section" id="the-mathematics-of-matrix-factorization-via-cgd">
<span id="index-3"></span><h4>The Mathematics of Matrix Factorization via CGD<a class="headerlink" href="#the-mathematics-of-matrix-factorization-via-cgd" title="Permalink to this headline">¶</a></h4>
<p>Matrix factorization by conjugate gradient descent produces ratings by using
the (limited) space of observed rankings to infer a user-factors vector
<img class="math" src="f_images/math/d369e06e2e85476bdf7909acafcd0c05132bb1ed.png" alt="p_{u}"/> for each user  <img class="math" src="f_images/math/e5fc41b391867da81606413e3389c7efc73abaf0.png" alt="u"/>, and an item-factors vector
<img class="math" src="f_images/math/59fa2a3d293bb76c78cac477a1dfa5abb61546c0.png" alt="q_{i}"/> for each item <img class="math" src="f_images/math/a581f053bbfa5115f42c13094857cdd12a37ec49.png" alt="i"/>, and then producing a ranking by user
<img class="math" src="f_images/math/e5fc41b391867da81606413e3389c7efc73abaf0.png" alt="u"/> of item <img class="math" src="f_images/math/a581f053bbfa5115f42c13094857cdd12a37ec49.png" alt="i"/> by the dot-product <img class="math" src="f_images/math/527189a0e01c3bf70f50ca5d9519f9aaa4bd2e43.png" alt="b_{ui} + p_{u}^{T}q_{i}"/>
where <img class="math" src="f_images/math/3ea2f1035270b8b8b548b0166b73653585f3f7ed.png" alt="b_{ui}"/> is a baseline ranking calculated as <img class="math" src="f_images/math/ffbbc0fcc85fb12bc5ffa677b4d34a629e73a258.png" alt="b_{ui} = \mu +
b_{u} + b_{i}"/>.</p>
<p>The optimum model is chosen to minimum the following sum, which penalizes
square distance of the prediction from observed rankings and complexity of the
model (through the regularization term):</p>
<div class="math">
<p><img src="f_images/math/985acba21aa7b6f593cceedb2f8f321d19c68908.png" alt="\sum_{(u,i) \in {\mathcal{K}}} (r_{ui} - \mu - b_{u} - b_{i} - \
p_{u}^{T}q_{i})^{2} + \lambda_{3}(||p_{u}||^{2} + ||q_{i}||^{2} + \
b_{u}^{2} + b_{i}^{2})"/></p>
</div><p>Where:</p>
<blockquote>
<div><div class="line-block">
<div class="line"><img class="math" src="f_images/math/4f38e6a7a2961a333be13278f57df633049554f9.png" alt="r_{ui}"/> – Observed ranking of item <img class="math" src="f_images/math/a581f053bbfa5115f42c13094857cdd12a37ec49.png" alt="i"/> by user <img class="math" src="f_images/math/e5fc41b391867da81606413e3389c7efc73abaf0.png" alt="u"/></div>
<div class="line"><img class="math" src="f_images/math/3c3f82017295b24c8595d5925016a20a24a4fce9.png" alt="{\mathcal{K}}"/> – Set of pairs <img class="math" src="f_images/math/7092db36c59ddc02ccec57d1658b010b5ba1c9b3.png" alt="(u,i)"/> for each observed
ranking of item <img class="math" src="f_images/math/a581f053bbfa5115f42c13094857cdd12a37ec49.png" alt="i"/> by user <img class="math" src="f_images/math/e5fc41b391867da81606413e3389c7efc73abaf0.png" alt="u"/></div>
<div class="line"><img class="math" src="f_images/math/126e84ba38f7dece5f0ad64e929b9588b20f6440.png" alt="\mu"/> – The average rating over all ratings of all items by all
users.</div>
<div class="line"><img class="math" src="f_images/math/bea714d7e21de291cb80b26c04185074e6546d00.png" alt="b_{u}"/> –  How much user <img class="math" src="f_images/math/e5fc41b391867da81606413e3389c7efc73abaf0.png" alt="u"/>&#8216;s average rating differs from
<img class="math" src="f_images/math/126e84ba38f7dece5f0ad64e929b9588b20f6440.png" alt="\mu"/>.</div>
<div class="line"><img class="math" src="f_images/math/61054babcfe66504db8061c3b435131e28d4cb5c.png" alt="b_{i}"/> –   How much item <img class="math" src="f_images/math/a581f053bbfa5115f42c13094857cdd12a37ec49.png" alt="i"/>&#8216;s average rating differs from
<img class="math" src="f_images/math/126e84ba38f7dece5f0ad64e929b9588b20f6440.png" alt="\mu"/></div>
<div class="line"><img class="math" src="f_images/math/d369e06e2e85476bdf7909acafcd0c05132bb1ed.png" alt="p_{u}"/> –  User-factors vector.</div>
<div class="line"><img class="math" src="f_images/math/59fa2a3d293bb76c78cac477a1dfa5abb61546c0.png" alt="q_{i}"/> – Item-factors vector.</div>
<div class="line"><img class="math" src="f_images/math/a23661a5e6ab31677cafdc3db349c930de9bce28.png" alt="\lambda_{3}"/> – A regularization parameter specified by the user.</div>
</div>
</div></blockquote>
<p>This optimization problem is solved by the conjugate gradient descent method.
Indeed, this difference in how the optimization problem is solved is the
primary difference between matrix factorization by CGD and matrix factorization
by ALS.</p>
</div>
<div class="section" id="comparison-between-cgd-and-als">
<h4>Comparison between CGD and ALS<a class="headerlink" href="#comparison-between-cgd-and-als" title="Permalink to this headline">¶</a></h4>
<p>Both CGD and ALS provide recommendation systems based on matrix factorization;
the difference is that CGD employs the conjugate gradient descent instead of
least squares for its optimization phase.
In particular, they share the same bipartite graph representation and the same
cost function.</p>
<ul class="simple">
<li>ALS finds a better solution faster - when it can run on the cluster it is
given.</li>
<li>CGD has slighter memory requirements and can run on datasets that can
overwhelm the ALS-based solution.</li>
</ul>
<p>When feasible, ALS is a preferred solver over CGD, while CGD is recommended
only when the application requires so much memory that it might be beyond the
capacity of the system.
CGD has a smaller memory requirement, but has a slower rate of convergence and
can provide a rougher estimate of the solution than the more computationally
intensive ALS.</p>
<p>The reason for this is that ALS solves the optimization problem by a least
squares that requires inverting a matrix.
Therefore, it requires more memory and computational effort.
But ALS, a 2nd-order optimization method, enjoys higher convergence rate and is
potentially more accurate in parameter estimation.</p>
<p>On the otherhand, CGD is a 1.5th-order optimization method that approximates
the Hessian of the cost function from the previous gradient information
through N consecutive CGD updates.
This is very important in cases where the solution has thousands or even
millions of components.</p>
<div class="section" id="usage">
<h5>Usage<a class="headerlink" href="#usage" title="Permalink to this headline">¶</a></h5>
<p>The matrix factorization by CGD procedure takes a property graph, encoding a
biparite user-item ranking network, selects a subset of the edges to be
considered (via a selection of edge labels), takes initial ratings from
specified edge property values, and then writes each user-factors vector to its
user vertex in a specified vertex property name and each item-factors vector to
its item vertex in the specified vertex property name.</p>
</div>
</div>
</div>
</div>
<div class="section" id="graphical-models">
<span id="id1"></span><h2>Graphical Models<a class="headerlink" href="#graphical-models" title="Permalink to this headline">¶</a></h2>
<p>The graphical models find more insights from structured noisy data.
See <a class="reference external" href="python_api/graphs/index.html">graph API</a> for details of the
<strong>Label Propagation (LP)</strong> and <strong>Loopy Belief Propagation (LBP)</strong>.</p>
</div>
<div class="section" id="topic-modeling">
<span id="id2"></span><h2>Topic Modeling<a class="headerlink" href="#topic-modeling" title="Permalink to this headline">¶</a></h2>
<p>For Topic Modeling, see: <a class="reference external" href="http://en.wikipedia.org/wiki/Topic_model">http://en.wikipedia.org/wiki/Topic_model</a></p>
<span class="target" id="lda"></span><div class="section" id="topic-modeling-with-latent-dirichlet-allocation">
<span id="index-4"></span><h3>Topic Modeling with Latent Dirichlet Allocation<a class="headerlink" href="#topic-modeling-with-latent-dirichlet-allocation" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="glossary.html#term-topic-modeling"><span class="xref std std-term">Topic modeling</span></a> algorithms are a class of statistical approaches to
partitioning items in a data set into subgroups.
As the name implies, these algorithms are often used on corpora of textual
data, where they are used to group documents in the collection into
semantically-meaningful groupings.
For an overall introduction to topic modeling, the reader might refer to the
work of David Blei and Michael Jordan, who are credited with creating and
popularizing topic modeling in the machine learning community.
In particular, Blei&#8217;s 2011 paper provides a nice introduction,
and is freely-available online <a class="footnote-reference" href="#lda1" id="id3">[1]</a> .</p>
<p id="index-5"><abbr title="Latent Dirichlet Allocation">LDA</abbr> is a commonly-used algorithm for topic modeling, but, more broadly,
is considered a dimensionality reduction technique.
It contrasts with other approaches (for example, latent semantic indexing), in
that it creates what&#8217;s referred to as a generative probabilistic model — a
statistical model that allows the algorithm to generalize its approach to topic
assignment to other, never-before-seen data points.
For the purposes of exposition, we&#8217;ll limit the scope of our discussion of
<abbr title="Latent Dirichlet Allocation">LDA</abbr> to the world of natural language processing, as it has an intuitive use
there (though <abbr title="Latent Dirichlet Allocation">LDA</abbr> can be used on other types of data).
In general, <abbr title="Latent Dirichlet Allocation">LDA</abbr> represents documents as random mixtures over topics in the
corpus.
This makes sense because any work of writing is rarely about a single subject.
Take the case of a news article on the President of the United States of
America&#8217;s approach to healthcare as an example.
It would be reasonable to assign topics like President, USA, health insurance,
politics, or healthcare to such a work, though it is likely to primarily
discuss the President and healthcare.</p>
<p><abbr title="Latent Dirichlet Allocation">LDA</abbr> assumes that input corpora contain documents pertaining to a given number
of topics, each of which are associated with a variety of words, and that each
document is the result of a mixture of probabilistic samplings: first over the
distribution of possible topics for the corpora, and second over the list of
possible words in the selected topic.
This generative assumption confers one of the main advantages <abbr title="Latent Dirichlet Allocation">LDA</abbr> holds over
other topic modeling approaches, such as probabilistic and regular <abbr title="Latent Semantic Indexing">LSI</abbr>.
As a generative model, <abbr title="Latent Dirichlet Allocation">LDA</abbr> is able to generalize the model it uses to
separate documents into topics to documents outside the corpora.
For example, this means that using <abbr title="Latent Dirichlet Allocation">LDA</abbr> to group online news articles into
categories like Sports, Entertainment, and Politics, it would be possible to
use the fitted model to help categorize newly-published news stories.
Such an application is beyond the scope of approaches like <abbr title="Latent Semantic Indexing">LSI</abbr>.
What&#8217;s more, when fitting an <abbr title="Latent Semantic Indexing">LSI</abbr> model, the number of parameters that have
to be estimated scale linearly with the number of documents in the corpus,
whereas the number of parameters to estimate for an <abbr title="Latent Dirichlet Allocation">LDA</abbr> model scales with the
number of topics — a much lower number, making it much better-suited to working
with large data sets.</p>
<div class="section" id="the-typical-lda-workflow">
<h4>The Typical <abbr title="Latent Dirichlet Allocation">LDA</abbr> Workflow<a class="headerlink" href="#the-typical-lda-workflow" title="Permalink to this headline">¶</a></h4>
<p>Although every user is likely to have his or her own habits and preferred
approach to topic modeling a document corpus, there is a general workflow that
is a good starting point when working with new data.
The general steps to the topic modeling with <abbr title="Latent Dirichlet Allocation">LDA</abbr> include:</p>
<ol class="arabic simple">
<li>Data preparation and ingest</li>
<li>Assignment to training or testing partition</li>
<li>Graph construction</li>
<li>Training <abbr title="Latent Dirichlet Allocation">LDA</abbr></li>
<li>Evaluation</li>
<li>Interpretation of results</li>
</ol>
</div>
<div class="section" id="data-preparation-and-ingest">
<h4>Data preparation and ingest<a class="headerlink" href="#data-preparation-and-ingest" title="Permalink to this headline">¶</a></h4>
<p>Most topic modeling workflows involve several data pre-processing and cleaning
steps.
Depending on the characteristics of the data being analyzed, there are
different best-practices to use here, so it&#8217;s important to be familiar with
the standard procedures for analytics in the domain from which the text
originated.
For example, in the biomedical text analytics community, it is common practice
for text analytics workflows to involve pre-processing for identifying negation
statements (Chapman et al., 2001 <a class="footnote-reference" href="#lda2" id="id4">[2]</a> ).
The reason for this is many analysts in that domain are examining text for
diagnostic statements — thus, failing to identify a negated statement in which
a disease is mentioned could lead to undesirable false-positives, but this
phenomenon may not arise in every domain.
In general, both stemming and stop word filtering are recommended steps for
topic modeling pre-processing.
Stemming refers to a set of methods used to normalize different tenses and
variations of the same word (for example, stemmer, stemming, stemmed, and
stem).
Stemming algorithms will normalize all variations of a word to one common form
(for example, stem).
There are many approaches to stemming, but the Porter Stemming (Porter, 2006
<a class="footnote-reference" href="#lda3" id="id5">[3]</a> ) is one of the most commonly-used.</p>
<p>Removing common, uninformative words, or stop word filtering, is another
commonly-used step in data pre-processing for topic modeling.
Stop words include words like <em>the</em>, <em>and</em>, or <em>a</em>, but the full list of
uninformative words can be quite long and depend on the domain producing the
text in question.
Example stop word lists online <a class="footnote-reference" href="#lda4" id="id6">[4]</a> can be a great place to start, but
being aware of the best-practices in the applicalbe field is necessary to
expand upon these.</p>
<p>There may be other pre-processing steps needed, depending on the type of text
being worked with.
Punctuation removal is frequently recommended, for example.
To determine what&#8217;s best for the text being analyzed, it helps to understand a
bit about what how <abbr title="Latent Dirichlet Allocation">LDA</abbr> analyzes the input text.
To learn the topic model, <abbr title="Latent Dirichlet Allocation">LDA</abbr> will typically look at the frequency of
individual words across documents, which are determined based on
space-separation.
Thus, each word will be interpreted independent of where it occurs in a
document, and without regard for the words that were written around it.
In the text analytics field, this is often referred to as a <em>bag of words</em>
approach to tokenization, the process of separating input text into
composite features to be analyzed by some algorithm.
When choosing pre-processing steps, it helps to keep this in mind.
Don&#8217;t worry too much about removing words or modifying their format — you&#8217;re
not manipulating your data!
These steps simply make it easier for the topic modeling algorithm to find the
latent topics that comprise your corpus.</p>
</div>
<div class="section" id="assignment-to-training-or-testing-partition">
<h4>Assignment to training or testing partition<a class="headerlink" href="#assignment-to-training-or-testing-partition" title="Permalink to this headline">¶</a></h4>
<p>The random assignment to training and testing partitions is an important step
in most every machine learning workflow.
It is common practice to withhold a random selection of one&#8217;s data set for the
purpose of evaluating the accuracy of the model that was learned from the
training data.
The results of this evaluation allow the user to confidently speak about the
generalizability of the trained model.
When speaking in these terms, be cautious that you only discuss
generalizability to the broader population from which your data was originally
obtained.
If a topic model is trained on neuroscience-related publications,
for example, evaluating the model on other neuroscience-related publications
is valid.
It would not be valid to discuss the model&#8217;s ability to work on documents from
other domains.</p>
<p>There are various schools of thought for how to assign a data set to training
and testing collections, but all agree that the process should be random.
Where analysts disagree is in the ratio of data to be assigned to each.
In most situations, the bulk of data will be assigned to the training
collection, because the more data that can be used to train the algorithm,
the better the resultant model will typically be.
It&#8217;s also important that the testing collection have sufficient data to
be able to reflect the characteristics of the larger
population from which it was drawn (this becomes an important issue when
working with data sets with rare topics, for example).
As a starting point, many people will use a 90%/10% training/test collection
split, and modify this ratio based on the characteristics of the documents
being analyzed.</p>
</div>
<div class="section" id="graph-construction">
<h4>Graph construction<a class="headerlink" href="#graph-construction" title="Permalink to this headline">¶</a></h4>
<p>Trusted Analytics uses a bipartite graph, to learn an <abbr title="Latent Dirichlet Allocation">LDA</abbr> topic model.
This graph contains vertices in two columns.
The left-hand column contains unique ids, each corresponding to a document in
the training collection, while the right-hand column contains unique ids
corresponding to each word in the entire training set, following any
pre-processing steps that were used.
Connections between these columns, or edges, denote the number of times a
particular word appears in a document, with the weight on the edge in question
denoting the number of times the word was found there.
After graph construction, many analysts choose to normalize the weights using
one of a variety of normalization schemes.
One approach is to normalize the weights to sum to 1, while another is to use
an approach called term frequency-inverse document frequency (tfidf), where the
resultant weights are meant to reflect how important a word is to a document in
the corpus.
Whether to use normalization — or what technique to use — is an open question,
and will likely depend on the characteristics of the text being analyzed.
Typical text analytics experiments will try a variety of approaches on a small
subset of the data to determine what works best.</p>
<p>See <a class="reference internal" href="#ds-mlal-lda-fig-1">ds_mlal_lda_fig_1</a>.</p>
<div class="figure align-center" id="id10">
<span id="ds-mlal-lda-fig-1"></span><img alt="f_images/ds_mlal_lda_1.png" src="f_images/ds_mlal_lda_1.png" />
<p class="caption"><span class="caption-text">Figure 1 - Example layout of a bipartite graph for LDA.</span></p>
<div class="legend">
The left-hand column contains one vertex for each document in the input
corpus, while the right-hand column contains vertices for each unique word
found in them.
Edges connecting left- and right-hand columns denote the number of times
the word was found in the document the edge connects.
The weights of the edges used in this example were not normalized.</div>
</div>
</div>
<div class="section" id="training-lda">
<h4>Training <abbr title="Latent Dirichlet Allocation">LDA</abbr><a class="headerlink" href="#training-lda" title="Permalink to this headline">¶</a></h4>
<p>In using <abbr title="Latent Dirichlet Allocation">LDA</abbr>, we are trying to model a document collection in terms of topics
<img class="math" src="f_images/math/6fc7854106dccedc657c7ab3b895f55bb035ce49.png" alt="\beta_{1:K}"/>, where each <img class="math" src="f_images/math/69b7d1c2cbc4553d99a5e37f8078d1422285d9f9.png" alt="\beta_{K}"/> describes a distribution
over the set of words in the training corpus.
Every document <img class="math" src="f_images/math/425d86ba2f2979d75b7535c2bcf92c33ed6b285a.png" alt="d"/>, then, is a vector of proportions <img class="math" src="f_images/math/ff94a45d114927f40dc83b74192969ff014e4305.png" alt="\theta_d"/>,
where <img class="math" src="f_images/math/0b423201cc87446c0fcc3fadcb387e3972c88d0e.png" alt="\theta_{d,k}"/> is the proportion of the <img class="math" src="f_images/math/85f8727408c03c41f79e3de553aeda83fce59d4c.png" alt="d^{th}"/> document for
topic <img class="math" src="f_images/math/e9203da50e1059455123460d4e716c9c7f440cc3.png" alt="k"/>.
The topic assignment for document <img class="math" src="f_images/math/425d86ba2f2979d75b7535c2bcf92c33ed6b285a.png" alt="d"/> is <img class="math" src="f_images/math/578bdc7cd948ad59d5e331724419de33d1bd554f.png" alt="z_{d}"/>, and
<img class="math" src="f_images/math/228f6bb401d445d6ebc5b184933b798590f549eb.png" alt="z_{d,n}"/> is the topic assignment for the <img class="math" src="f_images/math/e2a3d86e5a777d88623c55c60acfff4a30ff262d.png" alt="n^{th}"/> word
in document <img class="math" src="f_images/math/425d86ba2f2979d75b7535c2bcf92c33ed6b285a.png" alt="d"/>.
The words observed in document <img class="math" src="f_images/math/425d86ba2f2979d75b7535c2bcf92c33ed6b285a.png" alt="d"/> are :math&#8221;<cite>w_{d}</cite>, and
<img class="math" src="f_images/math/78b386c513bb6520bb26585e5570c21d56299647.png" alt="w_{d,n}"/> is the <img class="math" src="f_images/math/e2a3d86e5a777d88623c55c60acfff4a30ff262d.png" alt="n^{th}"/> word in document <img class="math" src="f_images/math/425d86ba2f2979d75b7535c2bcf92c33ed6b285a.png" alt="d"/>.
The generative process for <abbr title="Latent Dirichlet Allocation">LDA</abbr>, then, is the joint distribution of hidden and
observed values</p>
<div class="math">
<p><img src="f_images/math/bb0d00578db59cde3d28908f6372678c9eb103d8.png" alt="p(\beta_{1:K},\theta_{1:D},z_{1:D},w_{1:D} )=\prod_{i=1}^{K} p(\beta_i) \
\prod_{i=1}^{D} p(\theta_d) \left(\sideset{_{}^{}}{_{n=1}^N}\prod_{}^{} \
p\left(z_{d,n} | \theta_{d} \right)p\left(w_{d,n} | \beta_{1:K},z_{d,n} \
\right) \right)"/></p>
</div><p>This distribution depicts several dependencies: topic assignment
<img class="math" src="f_images/math/228f6bb401d445d6ebc5b184933b798590f549eb.png" alt="z_{d,n}"/> depends on the topic proportions <img class="math" src="f_images/math/ff94a45d114927f40dc83b74192969ff014e4305.png" alt="\theta_d"/>,
and the observed word <img class="math" src="f_images/math/78b386c513bb6520bb26585e5570c21d56299647.png" alt="w_{d,n}"/> depends on topic assignment
<img class="math" src="f_images/math/228f6bb401d445d6ebc5b184933b798590f549eb.png" alt="z_{d,n}"/> and all the topics <img class="math" src="f_images/math/6fc7854106dccedc657c7ab3b895f55bb035ce49.png" alt="\beta_{1:K}"/>, for example.
Although there are no analytical solutions to learning the <abbr title="Latent Dirichlet Allocation">LDA</abbr> model, there
are a variety of approximate solutions that are used, most of which are based
on Gibbs Sampling (for example, Porteous et al., 2008 <a class="footnote-reference" href="#lda5" id="id7">[5]</a> ).
The Trusted Analytics uses an implementation related to this.
We refer the interested reader to the primary source on this approach to learn
more (Teh et al., 2006 <a class="footnote-reference" href="#lda6" id="id8">[6]</a> ).</p>
</div>
<div class="section" id="evaluation">
<h4>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¶</a></h4>
<p>As with every machine learning algorithm, evaluating the accuracy of the model
that has been obtained is an important step before interpreting the results.
With many types of algorithms, the best practices in this step are
straightforward — in supervised classification, for example, we know the true
labels of the data being classified, so evaluating performance can be as simple
as computing the number of errors, calculating receiver operating
characteristic, or F1 measure.
With topic modeling, the situation is not so straightforward.
This makes sense, if we consider with <abbr title="Latent Dirichlet Allocation">LDA</abbr> we&#8217;re using an algorithm to blindly
identify logical subgroupings in our data, and we don&#8217;t <em>a priori</em> know the
best grouping that can be found.
Evaluation, then, should proceed with this in mind, and an examination of
homogeneity of the words comprising the documents in each grouping is often
done.
This issue is discussed further in Blei&#8217;s 2011 introduction to topic modeling
<a class="footnote-reference" href="#lda7" id="id9">[7]</a> .
It is of course possible to evaluate a topic model from a statistical
perspective using our hold-out testing document collection — and this is a
recommended best practice — however, such an evaluation does not assess the
topic model in terms of how they are typically used.</p>
</div>
<div class="section" id="interpretation-of-results">
<h4>Interpretation of results<a class="headerlink" href="#interpretation-of-results" title="Permalink to this headline">¶</a></h4>
<p>After running <abbr title="Latent Dirichlet Allocation">LDA</abbr> on a document corpus, users will typically examine the top
<img class="math" src="f_images/math/413f8a8e40062a9090d9d50b88bc7b551b314c26.png" alt="n"/> most frequent words that can be found in each grouping.
With this information, one is often able to use their own domain expertise to
think of logical names for each topic (this situation is analogous to the step
in principal components analysis, wherein statisticians will think of logical
names for each principal component based on the mixture of dimensions each
spans).
Each document, then, can be assigned to a topic, based on the mixture of topics
it has been assigned.
Recall that <abbr title="Latent Dirichlet Allocation">LDA</abbr> will assign each document a set of probabilities
corresponding to each possible topic.
Researchers will often set some threshold value to make a categorical judgment
regarding topic membership, using this information.</p>
<table class="docutils footnote" frame="void" id="lda1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[1]</a></td><td><a class="reference external" href="http://www.cs.princeton.edu/~blei/papers/Blei2011.pdf">http://www.cs.princeton.edu/~blei/papers/Blei2011.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="lda2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[2]</a></td><td><a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S1532046401910299">http://www.sciencedirect.com/science/article/pii/S1532046401910299</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="lda3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id5">[3]</a></td><td><a class="reference external" href="http://tartarus.org/~martin/PorterStemmer/index.html">http://tartarus.org/~martin/PorterStemmer/index.html</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="lda4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id6">[4]</a></td><td><a class="reference external" href="http://www.textfixer.com/resources/common-english-words.txt">http://www.textfixer.com/resources/common-english-words.txt</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="lda5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id7">[5]</a></td><td><a class="reference external" href="http://www.ics.uci.edu/~newman/pubs/fastlda.pdf">http://www.ics.uci.edu/~newman/pubs/fastlda.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="lda6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id8">[6]</a></td><td><a class="reference external" href="http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_511.pdf">http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_511.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="lda7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id9">[7]</a></td><td><a class="reference external" href="http://www.cs.princeton.edu/~blei/papers/Blei2011.pdf">http://www.cs.princeton.edu/~blei/papers/Blei2011.pdf</a></td></tr>
</tbody>
</table>
</div>
</div>
</div>
</div>


          </div>
        </div>
          </div>
        </div>
      </div>
    </div>

    <div class="container container-navbar-bottom">
      <div class="spc-navbar">
        
      </div>
    </div>
    <div class="container">
    <div class="footer">
    <div class="row-fluid">
    <ul class="inline pull-left">
      <li>
        &copy; Copyright 2015, Intel.
      </li>
      <li>
      Last updated on Sep 17, 2015.
      </li>
    </ul>
    </div>
    </div>
    </div>
  </body>
</html>