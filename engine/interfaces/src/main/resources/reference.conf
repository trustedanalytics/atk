# This file contains settings that should be considered global to the system,
# such as connection information for connecting to other systems.

# Configuration for particular plugins should go in the reference.conf of the
# project that provides the plugin.

launcher.configDirectory = ${PWD}/conf

trustedanalytics.atk {

  component.archives.interfaces {
    extra-classpath = [${launcher.configDirectory}]
  }

  #jdbc connector section
  jdbc.prefix = "jdbc"
  jdbc.url.splitter = ":"

  postgres.prefix = "postgresql"
  mysql.prefix = "mysql"
  sqlserver.prefix = "sqlserver"

  datastore {
    connection-mysql {
      host = ""
      port = 3306
      database = ""
      url = ${trustedanalytics.atk.jdbc.prefix}${trustedanalytics.atk.jdbc.url.splitter}${trustedanalytics.atk.mysql.prefix}${trustedanalytics.atk.jdbc.url.splitter}"//"${trustedanalytics.atk.datastore.connection-mysql.host}${trustedanalytics.atk.jdbc.url.splitter}${trustedanalytics.atk.datastore.connection-mysql.port}"/"${trustedanalytics.atk.datastore.connection-mysql.database}
      username = ""
      password = ""
    }

    connection-postgresql {
      host = ""
      port = 5432
      database = ""
      url = ${trustedanalytics.atk.jdbc.prefix}${trustedanalytics.atk.jdbc.url.splitter}${trustedanalytics.atk.postgres.prefix}${trustedanalytics.atk.jdbc.url.splitter}"//"${trustedanalytics.atk.datastore.connection-postgresql.host}${trustedanalytics.atk.jdbc.url.splitter}${trustedanalytics.atk.datastore.connection-postgresql.port}"/"${trustedanalytics.atk.datastore.connection-postgresql.database}
      username = ""
      password = ""
    }

    connection-sqlserver {
      host = ""
      port = 5432
      database = ""
      url = ${trustedanalytics.atk.jdbc.prefix}${trustedanalytics.atk.jdbc.url.splitter}${trustedanalytics.atk.postgres.prefix}${trustedanalytics.atk.jdbc.url.splitter}"//"${trustedanalytics.atk.datastore.connection-sqlserver.host}${trustedanalytics.atk.jdbc.url.splitter}${trustedanalytics.atk.datastore.connection-sqlserver.port}"/"${trustedanalytics.atk.datastore.connection-sqlserver.database}
      username = ""
      password = ""
    }

    #jdbc urls from datastore
    postgres.url = ${trustedanalytics.atk.datastore.connection-postgresql.url}
    mysql.url = ${trustedanalytics.atk.datastore.connection-mysql.url}
    sqlserver.url = ${trustedanalytics.atk.datastore.connection-sqlserver.url}
  }

  #Configuration for the IAT metastore
  metastore {

    #Connection information for an in-memory database that gets
    #cleared at startup, only suitable for testing
    connection-h2 {
      # H2 is a in-memory Java database useful for testing
      url = ${trustedanalytics.atk.jdbc.prefix}${trustedanalytics.atk.jdbc.url.splitter}"h2"${trustedanalytics.atk.jdbc.url.splitter}"mem"${trustedanalytics.atk.jdbc.url.splitter}"iatest;DB_CLOSE_DELAY=-1;LOCK_TIMEOUT=30000"
      driver = "org.h2.Driver"
      username = "" # leave blank, no user or password is needed for H2
      password = "" # leave blank, no user or password is needed for H2
    }

    #Connection information for Postgresql, suitable for normal use
    connection-postgresql {
      host = "invalid-postgresql-host"
      port = 5432
      database = "metastore"
      url = ${trustedanalytics.atk.jdbc.prefix}${trustedanalytics.atk.jdbc.url.splitter}${trustedanalytics.atk.postgres.prefix}${trustedanalytics.atk.jdbc.url.splitter}"//"${trustedanalytics.atk.metastore.connection-postgresql.host}":"${trustedanalytics.atk.metastore.connection-postgresql.port}"/"${trustedanalytics.atk.metastore.connection-postgresql.database}
      driver = "org.postgresql.Driver"
      username = "metastore"
      password = "Trusted123"
    }

    #jdbc urls
    postgres.url = ${trustedanalytics.atk.metastore.connection-postgresql.url}

    # Choose the connection to use. ${trustedanalytics.atk.metastore.connection-postgresql} and
    # ${trustedanalytics.atk.metastore.connection-h2} are valid values by default, others can
    # be added in application.conf if desired.
    connection = ${trustedanalytics.atk.metastore.connection-postgresql}

    pool {
      # max size of DB connection pool
      max-active = 100
    }
  }

  engine {

    page-size = 100000

    fs {
      # the system will create an "intelanalaytics" folder at this location.
      # All Trusted Analytics Toolkit files will be stored somewhere under that base location.
      root = "hdfs://invalid-fsroot-host/user/atkuser"
    }

    # New in-progress feature for keeping Yarn Application Master alive between requests.
    # This is a feature flag so we can begin adding code to the master branch without breaking
    # current application or needing long-lived feature branches
    keep-yarn-job-alive = false

    spark {
      # used to set the max number of threads in a custom execution context
      max-threads-per-execution-Context = 128

      # When master is empty the system defaults to spark://`hostname`:7070 where hostname is calculated from the current system
      master = "yarn-cluster"
      # When home is empty the system will check expected locations on the local system and use the first one it finds
      # If spark is running in yarn-cluster mode (spark.master = "yarn-cluster"), spark.home needs to be set to the spark directory on CDH cluster
      # ("/usr/lib/spark","/opt/cloudera/parcels/CDH/lib/spark/", etc)
      home = "/opt/cloudera/parcels/CDH/lib/spark/"

      # in cluster mode, set master and home like the example
      // master = "spark://MASTER_HOSTNAME:7077"
      // home = "/opt/cloudera/parcels/CDH/lib/spark"
      // home = "/usr/lib/spark"

      # local mode
      // master = "local[4]"

      # path to python worker execution, usually to toggle 2.6 and 2.7
      python-worker-exec = "python2.7"

      # directory storage for python udf dependencies on rest server
      python-udf-deps-directory = "/tmp/trustedanalytics/python_udf_deps/"

      python-default-deps-search-directories = [".", "..", "/usr/lib/trustedanalytics/rest-server/lib"]

      # Disable all kryo registration in plugins (this is mainly here for performance testing and debugging when
      # someone suspects Kryo might be causing some kind of issue).
      disable-kryo = true

      # Determines whether SparkContex.addJars() paths get "local:" prefix or not.
      # True if engine-core.jar, interfaces.jar and others are installed locally on each cluster node (preferred).
      # False is useful mainly for development on a cluster. False results in many copies of the application jars
      # being made and copied to all of the cluster nodes.
      app-jars-local = false

      #Placeholder for adding extra jars needed by the application. The commma separated strings should be in the format
      # similar to this: /opt/cloudera/parcels/CDH/jars/postgresql-9.1-901.jdbc4.jar
      extra-jars-for-spark-submit = ""

      # true to re-use a SparkContext, this can be helpful for automated integration tests, not for customers.
      reuse-context = false

      # Path to jar with LargeObjectTorrentBroadcastFactory which fixes bug in Spark when broadcast variables exceed 2GB.
      # TODO: Remove jar once fix is incorporated in Spark
      broadcast-factory-lib = "spark-broadcast-1.3.jar"

      conf {
        properties {
          # These key/value pairs will be parsed dynamica2ostlly and provided to SparkConf()
          # See Spark docs for possible values http://spark.apache.org/docs/0.9.0/configuration.html
          # All values should be convertible to Strings

          # Increased Akka frame size from default of 10MB to 100MB to allow tasks to send large results to Spark driver
          # (e.g., using collect() on large datasets)
          spark.akka.frameSize = 100

          # Use large object torrent broadcast to support broadcast variables larger than 2GB
          //spark.broadcast.factory= "org.apache.spark.broadcast.LargeObjectTorrentBroadcastFactory"

          # Limit of total size of serialized results of all partitions for each Spark action (e.g. collect).
          # Should be at least 1M, or 0 for unlimited. Jobs will be aborted if the total size is above this limit.
          # Having a high limit may cause out-of-memory errors in driver (depends on spark.driver.memory and memory overhead of objects in JVM).
          # Setting a proper limit can protect the driver from out-of-memory errors.
          spark.driver.maxResultSize = "1g"

          # Spark driver max PermGen Size. Currently the default of 64MB causes OOM issues in Spark Submit driver. So this
          # should be set higher than that. The following is added while launching the driver based on the value here:
          # --driver-java-options -XX:MaxPermSize=512m

          spark.driver.maxPermSize = "512m"

          //spark.core.connection.ack.wait.timeout=600
          //spark.akka.retry.wait=30000
          //spark.akka.timeout=30000

          # Memory should be same or lower than what is listed as available in Cloudera Manager
          spark.executor.memory = "8g"
          spark.hadoop.validateOutputSpecs = false

          spark.shuffle.consolidateFiles = true

          # Enabling RDD compression to save space (might increase CPU cycles)
          # Snappy compression is more efficient
          spark.rdd.compress = true
          spark.io.compression.codec = org.apache.spark.io.SnappyCompressionCodec

          //spark.storage.blockManagerHeartBeatMs=300000
          //spark.storage.blockManagerSlaveTimeoutMs=300000

          //spark.worker.timeout=30000

          // kryo should NOT be enabled here - it is enabled and configured on a per plugin basis.

          # To enable event logging, set spark.eventLog.enabled to true
          # and spark.eventLog.dir to the directory to which your event logs are written
          spark.eventLog.enabled = true
          spark.eventLog.overwrite = true
          //spark.eventLog.dir = "hdfs://invalid-spark-application-history-folder:8020/user/spark/applicationHistory"

        }

      }
    }

    #This section provides overrides to the default Hadoop configuration
    hadoop {
      #The path from which to load base configurations (e.g. core-site.xml would be in this folder)
      configuration.path = "/etc/hadoop/conf"
      kerberos {
        enabled = false
        principal-name = ""
        keytab-file = ""
      }
      mapreduce {
        job.user.classpath.first = true
        framework.name = "yarn"
      }
    }

    hbase {
      configuration.path = "/etc/hbase/conf"
    }

    hive {
      lib = "/opt/cloudera/parcels/CDH/lib/hive/lib:/opt/cloudera/parcels/CDH/lib/hive/lib/hive-exec-1.1.0-cdh5.4.2.jar:/opt/cloudera/parcels/CDH/lib/hive/lib/hive-metastore-1.1.0-cdh5.4.2.jar:/opt/cloudera/parcels/CDH/lib/hive/lib/libfb303-0.9.2.jar:/opt/cloudera/parcels/CDH/lib/hive/lib/jdo-api-3.0.1.jar:/opt/cloudera/parcels/CDH/lib/hive/lib/antlr-2.7.7.jar:/opt/cloudera/parcels/CDH/lib/hive/lib/antlr-runtime-3.4.jar"
      conf = "/etc/hive/conf"
    }

    jdbc {
      lib = "/opt/cloudera/parcels/CDH/jars/postgresql-9.1-901.jdbc4.jar"
    }


    giraph = ${trustedanalytics.atk.engine.hadoop}
    giraph {
      #Overrides of normal Hadoop settings that are used when running Giraph jobs
      giraph.maxWorkers = 1
      giraph.minWorkers = 1
      giraph.SplitMasterWorker = true
      //mapreduce.map.memory.mb = 4096
      //mapreduce.map.java.opts = "-Xmx3072m"
      giraph.zkIsExternal = false
      mapred.job.tracker = "not used" #not used - this can be set to anything but 'local' to make Giraph work
      archive.name = "giraph-plugins" #name of the plugin jar (without suffix) to launch
    }

    titan {
      load {
        # documentation for these settings is available on Titan website
        # http://s3.thinkaurelius.com/docs/titan/current/titan-config-ref.html
        storage {
          backend = "hbase"
          # with clusters the hostname should be a comma separated list of host names with zookeeper role assigned
          hostname = "invalid-titan-host"
          port = "2181"
          # We use HBase 0.98 api as Titan does not support HBase 1.0.0 yet
          # Do not override this property unless working on Titan
          hbase.compat-class = "com.thinkaurelius.titan.diskstorage.hbase.HBaseCompat0_98"

          # Whether to enable batch loading into the storage backend. Set to true for bulk loads.
          batch-loading = true

          # Size of the batch in which mutations are persisted
          buffer-size = 2048

          lock {
            #Number of milliseconds the system waits for a lock application to be acknowledged by the storage backend
            wait-time = 400

            #Number of times the system attempts to acquire a lock before giving up and throwing an exception
            retries = 15
          }

          hbase {
            # Pre-split settngs for large datasets
            region-count = 12
            compression-algorithm = "SNAPPY"
          }

          cassandra {
            # Cassandra configuration options
          }
        }

        ids {
          #Globally reserve graph element IDs in chunks of this size. Setting this too low will make commits
          #frequently block on slow reservation requests. Setting it too high will result in IDs wasted when a
          #graph instance shuts down with reserved but mostly-unused blocks.
          block-size = 300000

          #Number of partition block to allocate for placement of vertices
          num-partitions = 10

          #The number of milliseconds that the Titan id pool manager will wait before giving up on allocating a new block of ids
          renew-timeout = 150000

          #When true, vertices and edges are assigned IDs immediately upon creation. When false, IDs are assigned
          #only when the transaction commits. Must be disabled for graph partitioning to work.
          flush = true

          authority {
            #This setting helps separate Titan instances sharing a single graph storage
            #backend avoid contention when reserving ID blocks, increasing overall throughput.
            # The options available are:
            #NONE = Default in Titan
            #LOCAL_MANUAL = Expert feature: user manually assigns each Titan instance a unique conflict avoidance tag in its local graph configuration
            #GLOBAL_MANUAL = User assigns a tag to each Titan instance. The tags should be globally unique for optimal performance,
            #                but duplicates will not compromise correctness
            #GLOBAL_AUTO = Titan randomly selects a tag from the space of all possible tags when performing allocations.
            conflict-avoidance-mode = "GLOBAL_AUTO"

            #The number of milliseconds the system waits for an ID block reservation to be acknowledged by the storage backend
            wait-time = 300

            # Number of times the system attempts ID block reservations with random conflict avoidance tags
            # before giving up and throwing an exception
            #randomized-conflict-avoidance-retries = 10
          }
        }

        auto-partitioner {
          hbase {
            # Number of regions per regionserver to set when creating Titan’s HBase table
            regions-per-server = 2

            # Number of input splits for Titan reader is based on number of available cores as follows:
            #    Number of splits = Max(input-splits-per-spark-core *available spark-cores, HBase table region count),
            input-splits-per-spark-core = 2
          }

          enable = false
        }
      }


      query {
        storage {
          # query does use the batch load settings in titan.load
          backend = "hbase"
          hostname = "invalid-titan-query-host"
          port = 2181
          # We use HBase 0.98 api as Titan does not support HBase 1.0.0 yet
          # Do not override this property unless working on Titan
          hbase.compat-class = "com.thinkaurelius.titan.diskstorage.hbase.HBaseCompat0_98"
        }
        cache {
          # Adjust cache size parameters if you experience OutOfMemory errors during Titan queries
          # Either increase heap allocation for TrustedAnalytics Engine, or reduce db-cache-size
          # Reducing db-cache will result in cache misses and increased reads from disk
          db-cache = true
          db-cache-clean-wait = 20
          db-cache-time = 180000
          db-cache-size = 0.3 #Allocates 30% of available heap to Titan (default is 50%)
        }
      }
    }
  }
}
