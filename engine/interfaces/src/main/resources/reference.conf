# This file contains settings that should be considered global to the system,
# such as connection information for connecting to other systems.

# Configuration for particular plugins should go in the reference.conf of the
# project that provides the plugin.

launcher.configDirectory = ${PWD}/conf

trustedanalytics.atk {

  component.archives.interfaces {
    extra-classpath = [${launcher.configDirectory}]
  }

  #jdbc connector section
  jdbc.prefix = "jdbc"
  jdbc.url.splitter = ":"

  postgres.prefix = "postgresql"
  mysql.prefix = "mysql"
  sqlserver.prefix = "sqlserver"

  #Configuration for the IAT metastore
  metastore {

    #Connection information for an in-memory database that gets
    #cleared at startup, only suitable for testing
    connection-h2 {
      # H2 is a in-memory Java database useful for testing
      url = ${trustedanalytics.atk.jdbc.prefix}${trustedanalytics.atk.jdbc.url.splitter}"h2"${trustedanalytics.atk.jdbc.url.splitter}"mem"${trustedanalytics.atk.jdbc.url.splitter}"iatest;DB_CLOSE_DELAY=-1;LOCK_TIMEOUT=30000"
      driver = "org.h2.Driver"
      username = "" # leave blank, no user or password is needed for H2
      password = "" # leave blank, no user or password is needed for H2
    }

    #Connection information for Postgresql, suitable for normal use
    connection-postgresql {
      host = "invalid-postgresql-host"
      port = 5432
      database = "metastore"
      url = ${trustedanalytics.atk.jdbc.prefix}${trustedanalytics.atk.jdbc.url.splitter}${trustedanalytics.atk.postgres.prefix}${trustedanalytics.atk.jdbc.url.splitter}"//"${trustedanalytics.atk.metastore.connection-postgresql.host}":"${trustedanalytics.atk.metastore.connection-postgresql.port}"/"${trustedanalytics.atk.metastore.connection-postgresql.database}
      driver = "org.postgresql.Driver"
      username = "metastore"
      password = "Trusted123"
    }

    #jdbc urls
    postgres.url = ${trustedanalytics.atk.metastore.connection-postgresql.url}

    # Choose the connection to use. ${trustedanalytics.atk.metastore.connection-postgresql} and
    # ${trustedanalytics.atk.metastore.connection-h2} are valid values by default, others can
    # be added in application.conf if desired.
    connection = ${trustedanalytics.atk.metastore.connection-postgresql}

    pool {
      # max size of DB connection pool
      max-active = 100
    }
  }

  engine {

    page-size = 100000

    fs {
      # the system will create an "intelanalaytics" folder at this location.
      # All Trusted Analytics Toolkit files will be stored somewhere under that base location.
      root = "hdfs://invalid-fsroot-host/user/atkuser"

      # Directory to load checkpoints into
      checkpoint-directory = ${trustedanalytics.atk.engine.fs.root}"/checkpoints"
    }

    # New in-progress feature for keeping Yarn Application Master alive between requests.
    # This is a feature flag so we can begin adding code to the master branch without breaking
    # current application or needing long-lived feature branches
    keep-yarn-job-alive = true

    # After this many seconds Yarn job will be shutdown if there was no further activity
    yarn-wait-timeout = 300

    # Max time in minutes for a task to complete before aborting (the frequency of the YarnJobMonitor check)
    yarn-monitor-task-timeout = 20

    spark {
      # used to set the max number of threads in a custom execution context
      max-threads-per-execution-Context = 128

      # When master is empty the system defaults to spark://`hostname`:7070 where hostname is calculated from the current system
      master = "yarn-cluster"
      # When home is empty the system will check expected locations on the local system and use the first one it finds
      # If spark is running in yarn-cluster mode (spark.master = "yarn-cluster"), spark.home needs to be set to the spark directory on CDH cluster
      # ("/usr/lib/spark","/opt/cloudera/parcels/CDH/lib/spark/", etc)
      home = "/opt/cloudera/parcels/CDH/lib/spark"

      # dynamic libraries for Intel Data Analytics Acceleration Library (Intel DAAL)
      // daal.dynamic-libraries=${DAAL_LIB_DIR}"/libAtkDaalJavaAPI.so,"${DAAL_LIB_DIR}"/libiomp5.so,"${DAAL_LIB_DIR}"/libJavaAPI.so,"${DAAL_LIB_DIR}"/"${DAAL_GCC_VERSION}"/libtbb.so.2"

      # in cluster mode, set master and home like the example
      // master = "spark://MASTER_HOSTNAME:7077"
      // home = "/opt/cloudera/parcels/CDH/lib/spark"
      // home = "/usr/lib/spark"

      # local mode
      // master = "local[4]"

      # path to python worker execution, usually to toggle 2.6 and 2.7
      python-worker-exec = "python2.7"

      # directory storage for python udf dependencies on rest server
      python-udf-deps-directory = "/tmp/trustedanalytics/python_udf_deps/"

      python-default-deps-search-directories = [".", "..", "/usr/lib/trustedanalytics/rest-server/lib"]

      # Disable all kryo registration in plugins (this is mainly here for performance testing and debugging when
      # someone suspects Kryo might be causing some kind of issue).
      disable-kryo = true

      # Determines whether SparkContex.addJars() paths get "local:" prefix or not.
      # True if engine-core.jar, interfaces.jar and others are installed locally on each cluster node (preferred).
      # False is useful mainly for development on a cluster. False results in many copies of the application jars
      # being made and copied to all of the cluster nodes.
      app-jars-local = false

      #Placeholder for adding extra jars needed by the application. The commma separated strings should be in the format
      # similar to this: /opt/cloudera/parcels/CDH/jars/postgresql-9.1-901.jdbc4.jar
      extra-jars-for-spark-submit = ""

      # true to re-use a SparkContext, this can be helpful for automated integration tests, not for customers.
      reuse-context = false

      # Path to jar with LargeObjectTorrentBroadcastFactory which fixes bug in Spark when broadcast variables exceed 2GB.
      # TODO: Remove jar once fix is incorporated in Spark
      broadcast-factory-lib = "spark-broadcast-1.3.jar"

      conf {
        properties {
          # These key/value pairs will be parsed dynamica2ostlly and provided to SparkConf()
          # See Spark docs for possible values http://spark.apache.org/docs/0.9.0/configuration.html
          # All values should be convertible to Strings

          # Increased Akka frame size from default of 10MB to 100MB to allow tasks to send large results to Spark driver
          # (e.g., using collect() on large datasets)
          spark.akka.frameSize = 100

          # Use large object torrent broadcast to support broadcast variables larger than 2GB
          //spark.broadcast.factory= "org.apache.spark.broadcast.LargeObjectTorrentBroadcastFactory"

          # Limit of total size of serialized results of all partitions for each Spark action (e.g. collect).
          # Should be at least 1M, or 0 for unlimited. Jobs will be aborted if the total size is above this limit.
          # Having a high limit may cause out-of-memory errors in driver (depends on spark.driver.memory and memory overhead of objects in JVM).
          # Setting a proper limit can protect the driver from out-of-memory errors.
          spark.driver.maxResultSize = "1g"

          # Spark driver max PermGen Size. Currently the default of 64MB causes OOM issues in Spark Submit driver. So this
          # should be set higher than that. The following is added while launching the driver based on the value here:
          # --driver-java-options -XX:MaxPermSize=512m

          spark.driver.maxPermSize = "512m"

          //spark.core.connection.ack.wait.timeout=600
          //spark.akka.retry.wait=30000
          //spark.akka.timeout=30000

          # Memory should be same or lower than what is listed as available in Cloudera Manager
          spark.executor.memory = "8g"
          spark.hadoop.validateOutputSpecs = false

          spark.shuffle.consolidateFiles = true

          # Enabling RDD compression to save space (might increase CPU cycles)
          # Snappy compression is more efficient
          spark.rdd.compress = true
          spark.io.compression.codec = org.apache.spark.io.SnappyCompressionCodec

          //spark.storage.blockManagerHeartBeatMs=300000
          //spark.storage.blockManagerSlaveTimeoutMs=300000

          //spark.worker.timeout=30000

          // kryo should NOT be enabled here - it is enabled and configured on a per plugin basis.

          # To enable event logging, set spark.eventLog.enabled to true
          # and spark.eventLog.dir to the directory to which your event logs are written
          spark.eventLog.enabled = true
          spark.eventLog.overwrite = true
          //spark.eventLog.dir = "hdfs://invalid-spark-application-history-folder:8020/user/spark/applicationHistory"

          // Adding current library to extra library path for DAAL execution in Yarn
          spark.driver.extraLibraryPath = "."
          spark.executor.extraLibraryPath = "."

        }

      }
    }

    #This section provides overrides to the default Hadoop configuration
    hadoop {
      #The path from which to load base configurations (e.g. core-site.xml would be in this folder)
      configuration.path = "/etc/hadoop/conf"
      kerberos {
        enabled = false
        principal-name = ""
        keytab-file = ""
      }
      mapreduce {
        job.user.classpath.first = true
        framework.name = "yarn"
      }
    }

    hbase {
      configuration.path = "/etc/hbase/conf"
    }

    hive {
      lib = "/opt/cloudera/parcels/CDH/lib/hive/lib/hive-exec.jar:/opt/cloudera/parcels/CDH/lib/hive/lib/hive-metastore.jar:/opt/cloudera/parcels/CDH/lib/hive/lib/libfb303-0.9.2.jar:/opt/cloudera/parcels/CDH/lib/hive/lib/jdo-api-3.0.1.jar:/opt/cloudera/parcels/CDH/lib/hive/lib/antlr-2.7.7.jar:/opt/cloudera/parcels/CDH/lib/hive/lib/antlr-runtime-3.4.jar"
      conf = "/etc/hive/conf"
    }

    jdbc {
      lib = "/opt/cloudera/parcels/CDH/jars/postgresql-9.1-901.jdbc4.jar"
    }
  }
}
