trustedanalytics.atk {

  component.archives {
    engine-core {
      parent = "interfaces"
      class = "org.trustedanalytics.atk.engine.EngineApplication"
      config-path = "trustedanalytics.atk.engine"
    }
  }

  engine {
    logging {
      raw = false
      profile = false
    }
  }
}


#garbage collector configuration
trustedanalytics.atk.engine.gc {
  interval = "3 hours"
  stale-age = "7 days"  # if an entity has not been accessed for this duration, then the garbage collector will drop it
}

# Configuration for plugins and other things specific to this archive only
trustedanalytics.atk.engine {

  default-timeout = 30

  extra-classpath = [${trustedanalytics.atk.engine.hbase.configuration.path}, ${trustedanalytics.atk.engine.hadoop.configuration.path}]

  # Absolute local paths where jars are copied from to hdfs-lib
  //local-libs = [ "file:/usr/lib/trustedanalytics/rest-server/lib" ]
  local-libs = []

  # Path relative to fs.root where jars are copied to
  hdfs-lib = "/trustedanalytics/lib"

  auto-partitioner {
    # auto-partitioning takes a best guess based on the file size
    file-size-to-partition-size = [
      {upper-bound = "1MB", partitions = 30},
      {upper-bound = "1GB", partitions = 90},
      {upper-bound = "5GB", partitions = 200},
      {upper-bound = "10GB", partitions = 400},
      {upper-bound = "15GB", partitions = 750},
      {upper-bound = "25GB", partitions = 1000},
      {upper-bound = "50GB", partitions = 1500},
      {upper-bound = "100GB", partitions = 2000},
      {upper-bound = "200GB", partitions = 3000},
      {upper-bound = "300GB", partitions = 4000},
      {upper-bound = "400GB", partitions = 5000},
      {upper-bound = "600GB", partitions = 7500}
    ]

    # max-partitions is used if value is above the max upper-bound
    max-partitions = 10000

    repartition {
      # disabled - disable re-partitioning
      # frame_create_only - re-partition only during frame creation
      # shrink_only - re-partition during frame creation, and when the number partitions is less than existing partitions. Uses less-expensive Spark merge
      # shrink_or_grow - during frame creation, and either increase or decrease the number of partitions using more-expensive Spark shuffle
      #                  Using this option will also change the ordering of the frame during the shuffle
      strategy = "frame_create_only"

      # percentage change in number of partitions that triggers re-partition
      threshold-percent = 80

      # Used to estimate actual size of the frame for compressed file formats like Parquet.
      # This ratio prevents us from under-estimating the number of partitions for compressed files.
      # compression-ratio=uncompressed-size/compressed-size
      # e.g., compression-ratio=4 if  uncompressed size is 20MB, and compressed size is 5MB
      frame-compression-ratio = 3
    }

    # used by some Spark plugins to set the number of partitions to default-tasks-per-core * number of Spark cores
    default-tasks-per-core = 2

    # use broadcast join if file size is lower than threshold. zero disables broadcast joins.
    # this threshold should be less than the maximum size of results returned to Spark driver (i.e., spark.driver.maxResultSize).
    # to increase Spark driver memory, edit java options (IA_JVM_OPT) in /etc/default/trustedanalytics-rest-server
    broadcast-join-threshold = "512MB"
  }

  command {
    
    frames {
      load {
        config {
          # number of rows taken for sample test during frame loading
          schema-validation-sample-rows = 100

          # percentage of maximum rows fail in parsing in sampling test. 50 means up 50% is allowed
          schema-validation-fail-threshold-percentage = 50
        }
      }
    }
  }
}
